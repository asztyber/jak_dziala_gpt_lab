{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50a4c4a-4063-4e81-9275-fe326591175b",
   "metadata": {},
   "source": [
    "### Laboratorium 7 Jak działa GPT?\n",
    "#### Implemetacja transformera część 2\n",
    "\n",
    "Imię i nazwisko: ...................\n",
    "\n",
    "Punktacja:\n",
    "* 6 pkt. prawidłowa implementacja \n",
    "* 2 pkt. wnioski\n",
    "\n",
    "Rozszerzymy implementację z poprzedniego laboratorium o:\n",
    "* kodowanie pozycyjne\n",
    "* warstwę MLP za atencją (co razem daje blok transformera)\n",
    "* wiele bloków transformera\n",
    "* strumień resztowy (residual stream)\n",
    "* normalizację LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9418073-0cfe-4f4b-9536-643d45e69784",
   "metadata": {},
   "source": [
    "#### Źródła\n",
    "* https://youtu.be/kCc8FmEb1nY?si=wYbFi5JB3x-R8375\n",
    "* https://github.com/karpathy/nanoGPT\n",
    "* https://arena-chapter1-transformer-interp.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da88d5f-9e38-4be9-8959-0b44ec6a179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7068-c900-43a1-8a36-31fb08fd2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339a8713-e0a1-4ecc-b013-0eb1106f032e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to jest do assertów, proszę zignorować\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f33ff-5d83-4a24-bf24-48f382b5ecd6",
   "metadata": {},
   "source": [
    "#### Przygotowanie danych uczących\n",
    "* to samo co do tej pory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906b538-46d3-45a7-88f6-7e96d3cff217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jako tekst ponownie wykorzystamy HPMOR rozdziały 1-10\n",
    "# Eliezer Yudkowsky, Harry Potter and the Methods of Rationality https://hpmor.com/\n",
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/hpmor_chapters_1-10.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82951980-6ae4-4486-8549-e84d929ca6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78419332-d6a0-48ac-967d-3a607e2ed2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(set(text))\n",
    "vocab_size = len(characters)\n",
    "idx_to_ch = {i: c for i, c in enumerate(characters)}\n",
    "ch_to_idx = {c: i for i, c in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6833c1e-80cc-48b9-b130-dd3b68ffaba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2385f-bca7-4f5e-99fb-ee57cc84bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code(text):\n",
    "    return [ch_to_idx[c] for c in text]\n",
    "\n",
    "def decode(tokens):\n",
    "    return ''.join(idx_to_ch[i] for i in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dacb3f-14d8-484d-a6de-3422ed029c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_coded = code(text)\n",
    "# tensor zawierający dane uczące\n",
    "train = torch.tensor(text_coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4423b-8847-4791-9794-ea37619a838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len=8, batch_size=4):\n",
    "    '''\n",
    "    Funkcja zwraca batch danych.\n",
    "    data (tensor) - dane uczące\n",
    "    seq_len (int) - długość sekwencji\n",
    "    batch_size (int) - rozmiar batcha\n",
    "\n",
    "    X - tensor o kształcie (batch_size, seq_len)\n",
    "    y - tensor o kształcie (batch_size, seq_len)\n",
    "    '''\n",
    "    n = len(data)\n",
    "    starts = np.random.randint(0, n - seq_len, batch_size)\n",
    "    X = torch.stack([data[s:s + seq_len] for s in starts])\n",
    "    y = torch.stack([data[s + 1: s + seq_len + 1] for s in starts])\n",
    "    return X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac8c47-23d7-432d-84f0-26b26f3dc419",
   "metadata": {},
   "source": [
    "#### Generacja tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0269815-f673-445c-a7fc-032f389072d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_seq, model, max_size, seq_len):\n",
    "    '''\n",
    "    Funkcja generuje tekst.\n",
    "    start_seq (str) - początek tekstu, podany przez użytkownika\n",
    "    model - sieć neuronowa\n",
    "    max_size (int) - zadana długość tekstu\n",
    "    seq_len (int) - długość sekwencji podawanej na wejście modelu\n",
    "    '''\n",
    "    for i in range(max_size):\n",
    "        x = code(start_seq[-seq_len:]) #<- zmiana\n",
    "        logits = model(torch.tensor([x], device=device)) #<- zmiana []\n",
    "        probs = F.softmax(logits, dim=-1) #<- zmiana dim=-1\n",
    "        probs = probs[0, -1, :].cpu().detach().numpy() # <- zmiana\n",
    "        next_ch = idx_to_ch[np.random.choice(vocab_size, p=probs)]\n",
    "        start_seq += next_ch\n",
    "    return start_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963e9da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size):\n",
    "    losses = []\n",
    "    for step in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = get_batch(train, batch_size=batch_size)\n",
    "        logits = model(x)\n",
    "        loss = #TODO (uzupełnić z zeszłego tygodnia)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Krok {step}: Loss = {loss.item():.4f}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcfd110",
   "metadata": {},
   "source": [
    "### Potrzebne modele z zeszłego tygodnia\n",
    "* proszę skopiować implementację AttentionHead i MultiHeadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ad4f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9557f01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f15abb-c66d-448e-98e1-64d0423f6959",
   "metadata": {},
   "source": [
    "### Kodowanie pozycyjne (Positional embedding)\n",
    "\n",
    "* uwaga: na wejściu tej warstwy nie ma tokenów, tylko ich **numery** kolejne\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/positional_embedding.png?raw=true\" alt=\"Atencja o wielu głowach\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365bbec3",
   "metadata": {},
   "source": [
    "### Uzupełnić model o kodowanie pozycyjne\n",
    "* dodać warstwę kodowania pozycyjnego typu nn.Embedding o wymiarach (*seq_len*, *d_model*)\n",
    "* kodowanie pozycyjne **dodajemy** do wyniku warstwy emb\n",
    "* na wejściu warstwy kodowania pozycyjnego podajemy numery kolejne sekwencji tokenów (np. [[0, 1, 2, 3, 4, 5, 6, 7]])\n",
    "* tensor na wejściu ma kształt (*1*, *seq_len*)\n",
    "* proszę pamiętać o przeniesieniu tensora wejściowego do warstwy pos_emb na urządzenie device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07599f62-88f9-4a39-a3a1-9ff77f7f5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadModelwithPositionalEmbedding(nn.Module):\n",
    "    def __init__(self, n_heads, vocab_size, d_head, d_model, seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model) \n",
    "        self.pos_emb =  #TODO\n",
    "        self.multi_head_attention = MultiHeadAttention(n_heads, d_head, d_model)\n",
    "        self.linear_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        _, seq_len = X.shape # sprawdzamy długość sekwencji w danych wejściowych (model może działać na danych krótszych niż seq_len)\n",
    "        positional_embedding = #TODO\n",
    "        x = #TODO\n",
    "        out = self.multi_head_attention(x)\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7e667-43fd-4e2b-ae7c-6e26235fab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(37)\n",
    "batch_size = 256\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 4\n",
    "n_heads = 4\n",
    "n_steps = 2000\n",
    "model = MultiHeadModelwithPositionalEmbedding(n_heads, vocab_size, d_head, d_model, seq_len).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56cfa8ee-8722-4333-a4f8-44a465228267",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeec94da-6119-4dfc-9d8b-624e100c8786",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([256, 8, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ccaed8-eb39-4239-b254-54d166d39f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([-0.1740, -0.0442, -0.2145, -0.1074, -0.0305,  0.0535,  0.0226, -0.1520], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa9ba8-bc2d-4723-a872-7ae07906ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Liczba parametrów modelu: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b24fc-1ae4-4e30-b78a-ab142960d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474de3b9-db8d-4411-82ab-0f02116bfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897978d6-2bc7-4fa0-b29e-d44e1088de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b1ec5-61e6-47e2-a829-a6b8ec9f3fb6",
   "metadata": {},
   "source": [
    "### Transformer\n",
    "* tworzymy **TransformerBlock** dodając za atencją:\n",
    "    * warstwę liniową *d_model* x 4*d_model*\n",
    "    * funkcję aktywacji GELU\n",
    "    * warstwę liniową 4*d_model* x *d_model*\n",
    "\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/transformer_block.png?raw=true\" alt=\"Transformer\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451e6f20",
   "metadata": {},
   "source": [
    "### Zaimplementować klasę MLPBlock\n",
    "* warstwa liniowa *d_model* x 4*d_model*\n",
    "* funkcja aktywacji GELU\n",
    "* warstwa liniowa 4*d_model* x *d_model*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f015e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.linear1 = #TODO\n",
    "        self.gelu = #TODO\n",
    "        self.linear2 = #TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd016a88",
   "metadata": {},
   "source": [
    "### Zaimplementować klasę TransformerBlock\n",
    "* warstwa MultiHeadAttention\n",
    "* warstwa MLPBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f580179",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_head, d_model):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = #TODO\n",
    "        self.mlp = #TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137b036d",
   "metadata": {},
   "source": [
    "### Zaimplementować model z jedną warstwą TransformerBlock - TransformerOneLayerModel\n",
    "* warstwa embedding\n",
    "* warstwa kodowania pozycyjnego\n",
    "* warstwa TransformerBlock\n",
    "* warstwa liniowa *d_model* x *vocab_size*\n",
    "\n",
    "**Jedyna różnica w porównaniu do MultiHeadModelwithPositionalEmbedding to zamiana MultiHeadAttention na TransformerBlock**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b125688-885f-4b6a-8038-2225194050c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TransformerOneLayerModel(nn.Module):\n",
    "    def __init__(self, n_heads, vocab_size, d_head, d_model, seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model) \n",
    "        self.pos_emb = #TODO\n",
    "        self.transformer_block =  #TODO\n",
    "        self.linear_out = nn.Linear(d_model, vocab_size) \n",
    "\n",
    "    def forward(self, X):\n",
    "        _, seq_len = X.shape\n",
    "        x = #TODO\n",
    "        out = #TODO\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c65070-e0d5-4cf9-b0c0-5c149a8627d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(21)\n",
    "batch_size = 256\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 4\n",
    "n_heads = 4\n",
    "n_steps = 2000\n",
    "model = TransformerOneLayerModel(n_heads, vocab_size, d_head, d_model, seq_len).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9233e-f275-427d-92eb-8fac828e6f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fa7ff2-0145-40b7-9cbf-47c9771eb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([256, 8, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967fcb62-d20e-48e3-bf35-6d5d7324743f",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([0.1033, 0.1294, 0.1507, 0.1180, 0.1355, 0.1640, 0.1776, 0.1937], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "179ffef6-aee3-4711-af0f-66982ee14077",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Liczba parametrów modelu: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3ec161-c1ef-44ee-8ccc-ee5aadda0228",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422230aa-2bb9-4ec9-a9f8-ae58496a1895",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de69942-d661-4c06-8719-3f3fedfe28a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0be24e-43cf-40c6-980d-6891176dd458",
   "metadata": {},
   "source": [
    "### Wiele warstw - zaimplementować model TransformerModel\n",
    "* dodać 3 warstwy TransformerBlock po sobie\n",
    "* wykorzystać nn.Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f1942a-0375-4612-8049-08786befe187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer blocks zamiast transformer block\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, n_heads, vocab_size, d_head, d_model, seq_len):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, d_model) \n",
    "        self.pos_emb = #TODO\n",
    "        self.transformer_blocks = #TODO\n",
    "        self.linear_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        _, seq_len = X.shape\n",
    "        x = #TODO\n",
    "        out = #TODO\n",
    "        logits = self.linear_out(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cf679-083f-4126-94cb-25b7c07b1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(18)\n",
    "batch_size = 256\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 4\n",
    "n_heads = 4\n",
    "n_steps = 2000\n",
    "model = TransformerModel(n_heads, vocab_size, d_head, d_model, seq_len).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8a8bb6-0697-4726-9cc8-76fbae902311",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f39d76c-5213-44dc-bcfa-8e9f12302857",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([256, 8, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ed77b6-0725-4454-bd15-8403be7beec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([0.1095, 0.1096, 0.1095, 0.1095, 0.1095, 0.1095, 0.1095, 0.1095], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04cc0c5-59dd-49e0-8021-9cfefcd10166",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Liczba parametrów modelu: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6ef87a-799c-4228-8603-1dde4b4ceab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fa46b7-77ee-4432-a315-5d1470ea8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285bc7d8-c5e7-4639-9443-80708a6ed5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3f9422-a628-43bc-9a9f-86b32a8a47e3",
   "metadata": {},
   "source": [
    "### Residual stream\n",
    "* zmodyfikować transformer block, tak, żeby zawierał residual stream\n",
    "* $+$ na schemacie oznacza zwykłe dodawanie\n",
    "\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/residual_stream.png?raw=true\" alt=\"Residual stream\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6bec63",
   "metadata": {},
   "source": [
    "#### Zmodyfikować TransformerBlock\n",
    "* dodać residual stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3113c2-6a69-49a4-9ee4-3deec8ee939d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_head, d_model):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = #TODO\n",
    "        self.mlp = #TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = #TODO\n",
    "        x = #TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166731cb-c621-4d10-aeb9-d8612faca91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(73)\n",
    "batch_size = 256\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 4\n",
    "n_heads = 4\n",
    "n_steps = 2000\n",
    "model = TransformerModel(n_heads, vocab_size, d_head, d_model, seq_len).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49212f2e-e7c9-4a99-a9c5-843b643e2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "113e8935-ad1f-41cb-b791-bc1db583ec56",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([256, 8, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6e4fcb-394d-48f9-ba76-cfe25fc4f479",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([-0.2209, -0.2540, -0.0918,  0.6298, -1.2298,  1.7620,  0.6835, -0.1646], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4c8a8f-89d9-4ba7-ba65-9ae72e10a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886b5c1-d0f5-4988-aa10-b4ba83f30a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8353a511-7613-4779-9d1e-cd1fbddf2370",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0ee5fa-4689-4daf-8e41-bf992f688349",
   "metadata": {},
   "source": [
    "### Layer norm\n",
    "* uzupełnić Transformer Block o dwie warstwy Layer Norm\n",
    "\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/layer_norm.png?raw=true\" alt=\"Layer norm\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cac8649",
   "metadata": {},
   "source": [
    "#### Zmodyfikować TransformerBlock\n",
    "* dodać dwie warstwy Layer Norm w miejscach według schematu (nn.LayerNorm)\n",
    "* do konstruktora warstwy LayerNorm podajemy rozmiar wymiaru, dla którego ma być zastosowana normalizacja (*d_model*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff3aff7-332e-41a8-887c-92d8b02d09db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, n_heads, d_head, d_model):\n",
    "        super().__init__()\n",
    "        self.multi_head_attention = #TODO\n",
    "        self.mlp = #TODO\n",
    "        self.ln1 = #TODO\n",
    "        self.ln2 = #TODO\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623c5f6e-e01b-469d-91a8-a72598a7627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(10)\n",
    "batch_size = 256\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 4\n",
    "n_heads = 4\n",
    "n_steps = 2000\n",
    "model = TransformerModel(n_heads, vocab_size, d_head, d_model, seq_len).to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00479e1-e84a-43d6-b5a3-fd692f240268",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd060466-132d-4abf-8a7b-da5afee3cf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([256, 8, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb875f49-4d4f-4c70-b52d-b044b32d7273",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([ 0.3888,  0.4468,  0.7320, -0.4866,  0.6700,  0.3762,  0.4792,  0.6207], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728bcfba-7402-4943-a489-d721d0878eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce5faeb-00a3-436a-b881-414f561f93b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa0a35-5b05-43fd-b034-a113d6ad86a1",
   "metadata": {},
   "source": [
    "### Wnioski\n",
    "1. Czy modele będą działać dla danych wejściowych dłuższych niż seq_len? Dlaczego? *Uwaga: w funkcji generate_text dane wejściowe są zawsze przycinane do seq_len*\n",
    "2. Porównaj wszystkie modele pod względem jakości tekstu, wartości funkcji straty i liczby parametrów\n",
    "3. (dla chętnych) Można eksperymentować z batch_size, seq_len, d_model, d_head, n_heads, n_steps, lr i liczbą warstw. Jaki najlepszy model udało się uzyskać?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8d7ee-00e2-496a-9f7d-d63e58d9375f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6d58c4ad-43ba-4646-b9cd-57b5ad6a17ce",
   "metadata": {},
   "source": [
    "### Dalsze możliwości rozwoju\n",
    "1. Wykorzystanie danych walidacyjnych (np. kolejnego rozdziału), aby sprawdzić, czy model się nie przeucza\n",
    "2. Zapis i odczyt checkpointów\n",
    "3. Zmniejszanie współczynnika uczenia w kolejnych krokach (learning rate decay)\n",
    "4. Więcej warstw, większe wymiary modelu\n",
    "5. Większy słownik (tokenizacja!)\n",
    "6. Obecnie każda głowa jest liczona osobo i wyniki są konkatenowane (a następnie sumowane). Dla wydajności można dodać liczbę głów jako czwarty wymiar. To podjeście jest równoważne matematycznie, jest wydajniejszą implementacją, ale jest trudnejsze do zrozumienia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0aa541-7e74-4bfb-bc0a-9b7cf78e29a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
