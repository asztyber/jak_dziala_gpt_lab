{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e98d4931-858b-4719-af61-4b6532f65426",
   "metadata": {},
   "source": [
    "## Osadzenia wektorowe (vector embeddings)\n",
    "\n",
    "Imię i nazwisko:\n",
    "\n",
    "Celem laboratorium jest:\n",
    "* Zapoznanie z się z koncepcją zamiany tekstu na wektory\n",
    "* Implementacja algorytmu CBOW\n",
    "\n",
    "Punktacja:\n",
    "* działający kod 6 pkt.\n",
    "* wnioski 2 pkt.\n",
    "\n",
    "Mikolov, Tomas, et al. \"Efficient estimation of word representations in vector space.\" arXiv preprint arXiv:1301.3781 (2013).\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/cbow.png?raw=true\" alt=\"Algorytm CBOW\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fef7005-9fe9-4e3e-ab24-08aa5a0922af",
   "metadata": {},
   "source": [
    "#### Dane uczące\n",
    "Zbiór danych WikiText2 (wybrane artykuły z Wikipedii)\n",
    "* Więcej tu: https://blog.salesforceairesearch.com/the-wikitext-long-term-dependency-language-modeling-dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd04efb-1a26-4953-8dc8-c8763997f834",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530b348a-9277-4e56-94f8-0ccdf6a47776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import nltk # podstawowa biblioteka do przetwarzania języka naturalnego\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f90b15-02b9-4962-b603-3fa3991ea069",
   "metadata": {},
   "source": [
    "### Przygotowanie danych uczących"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ed94ca-21ee-499f-86db-096faab1248a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"Salesforce/wikitext\", \"wikitext-2-raw-v1\", split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b35c2-edef-40d0-b2f5-55a4a012701d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a9ca1-99ab-454d-b46e-f004323061c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liczba linii tekstu w danych\n",
    "len(ds['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9459741f-b5a6-4a3c-bcfb-c785c3079113",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ds['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece919b0-d438-400a-b739-e109c4545ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf0b67d-d065-4159-b368-5021e68aedc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wszystko poza literami i cyframi zamieniamy na spacje za pomocą wyrażeń regularnych\n",
    "text = [re.sub(r'[^a-zA-Z0-9]', ' ', line) for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195b1e4-ad00-47fc-b47d-2e993868bcc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9746c10-5ad3-4a86-a777-a86027068624",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wszystkie litery zamieniamy na małe\n",
    "text = [line.lower() for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce7d3bf-f636-45a6-89f5-1edc41afdacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71eac9b4-0c4d-42b7-a9a6-32f1862387eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# każdą linię tekstu dzielimy na słowa (tokenizacja)\n",
    "text = [nltk.word_tokenize(line) for line in text]\n",
    "# i zostawiamy tylko linijki od długości przynajmniej 5 słów\n",
    "text = [line for line in text if len(line) > 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11573be-8659-4504-bd4e-a6526f6f9d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553747dd-dde4-4f09-9628-1a5c270ca5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lista słów (już bez podziału na linijki)\n",
    "words = [word for line in text for word in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abefa3-d9dd-4a5f-a80a-f44f79a13d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(words[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60234bc5-1534-4482-a67d-19218d9fc2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zliczamy słowa\n",
    "word_frequencies = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88466f-9164-484e-bc54-a0c4cf29f280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 najpopularniejszych słów i liczby wystąpień\n",
    "word_frequencies.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2651020-33ae-433d-b8b9-d7dbcaba1c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 najmniej popularnych słów i liczby wystąpień\n",
    "word_frequencies.most_common()[:-21:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ba4a35-f04e-4d93-90fb-8a02dbc8ad68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# liczba unikalnych słów\n",
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67255023-aecc-40e1-b6ac-c6f61b55070f",
   "metadata": {},
   "source": [
    "Nie chcemy uczyć zanurzeń wektorowych dla bardzo rzadkich słów (jeśli słowo wystąpiło tylko raz, to czego model może się nauczyć?)\n",
    "* stosujemy specjalny token UNK (jak unknown), którym zastąpimy rzadkie słowa\n",
    "* słowa, które wystąpiły w tekście mniej niż 50 razy zastępujemy tokenem UNK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff3a59d-1be8-46d8-9cb1-ee4a203ffae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [[word if word_frequencies[word] > 50 else \"UNK\" for word in line ] for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438c2e38-b5fe-4882-b120-c5518b86a194",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e89a1bd-4320-4f4a-bcdb-fd80e2632fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jeszcze raz lista słów bez podziału na linijki\n",
    "words = [word for line in text for word in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c912c749-9ef9-49d4-a3aa-9be6abc1c328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy vocab - listę unikalnych słów, posortowanych alfabetycznie\n",
    "vocab = sorted(set(words))\n",
    "# umieszczamy token UNK, tak, żeby miał indeks 0, ponieważ jest specyficzny\n",
    "vocab.remove('UNK')\n",
    "vocab = ['UNK'] + vocab\n",
    "vocab_size = len(vocab)\n",
    "print('Liczba unikalnych słów: ', vocab_size)\n",
    "print('Pierwsze (alfabetycznie) słowa:\\n', vocab[:10])\n",
    "print('Ostatnie (alfabetycznie) słowa:\\n', vocab[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0edc2a5-2442-4f8f-879f-beff78c1018a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# słownik słowo -> indeks (token)\n",
    "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
    "# słownik indeks -> słowo\n",
    "idx_to_word = {i: word for i, word in enumerate(vocab)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30203871-dad0-4eb7-a4a1-7349c201cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# początek słownika word_to_idx\n",
    "{k: word_to_idx[k] for k in list(word_to_idx)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee7e754-1a1d-41f6-b4ca-11b988348107",
   "metadata": {},
   "outputs": [],
   "source": [
    "# początek słownika idx_to_word\n",
    "{k: idx_to_word[k] for k in list(idx_to_word)[:10]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "578603ee-a883-47c8-a275-e6cc98720180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tekst zamieniony na tokeny (indeksy)\n",
    "tokenized_text = [[word_to_idx[word] for word in line] for line in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244e6046-644a-4347-a6f3-db3491a62fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128c4f97-8992-4c04-925a-8083e61c67e2",
   "metadata": {},
   "source": [
    "## Implementacja algorytmu CBOW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346eb9fc-8f34-43a0-88b0-cb2e5ce22ba7",
   "metadata": {},
   "source": [
    "#### Przygotowanie danych wejściowych"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e78cf7-7073-45e1-85ab-4b5216a1beab",
   "metadata": {},
   "source": [
    "* Będziemy uczyć model do predykcji słowa na podstawie jego kontekstu (słów z otoczenia).\n",
    "* Zastosujemy długość kontekstu 2\n",
    "* Co oznacza, że przewidujemy słowo na podstawie dwóch poprzednich i dwóch następnych słów\n",
    "\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/kontekst.png?raw=true\" alt=\"Kontekst\" width=\"500\" height=\"300\">\n",
    "\n",
    "Dane uczące dla modelu będą wyglądać tak (tylko operujemy na tokenach, a nie na słowach):\n",
    "\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/cbow_training.png?raw=true\" alt=\"Dane wejściowe i wyjściowe\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118ec77c-886f-45be-81f4-20019545a207",
   "metadata": {},
   "source": [
    "#### Funkcja generująca dane do uczenia\n",
    "* Na podstawie korpusu tekstu po tokenizacji generujemy dane do uczenia\n",
    "* Zwracana zmienna **data** jest listą krotek, zawierających (lista słów z kontekstu, słowo do przewidzenia (target))\n",
    "* Uzupełnij wybieranie indeksów dla słów z kontekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfbc0dd-84ac-4f8b-aa9c-0052c3cdc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_training_data(tokenized_text, context_len):\n",
    "    data = []\n",
    "    # dla każdego zdania\n",
    "    for sentence in tokenized_text:\n",
    "        sentence_len = len(sentence) # długość zdania\n",
    "        # uwaga: wewnątrz zadania iterujemy się tylko przez słowa, dla których jest odpowiedni kontekst\n",
    "        # tj. są przynajmniej dwa słowa przed i dwa słowa za\n",
    "        for i, word in enumerate(sentence[context_len:-context_len]):\n",
    "            idx = i + context_len # indeks aktualnego słowa\n",
    "            context_indices = #TODO wybierz indeksy odpowiadające słowom z kontekstu\n",
    "            \n",
    "            data.append(([sentence[ci] for ci in context_indices], word))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bccf267-00f3-42bf-a3e2-356a68b5f8a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = generate_training_data(tokenized_text, context_len=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc99afe5-6e13-44e3-912b-56e79ba958c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429f637e-dd93-41be-a687-2e0cb1f17def",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cb0911-1e43-40bd-ba7e-712b2b70e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fac3d0b-44bb-4463-8631-b36d93ece08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert training_data[:3] == [([0, 2481, 165, 0], 3773), ([2481, 3773, 0, 829], 165), ([3773, 165, 829, 2007], 0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251f262f-bb7c-4821-bfb1-25e81b1e9fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=8):\n",
    "    '''\n",
    "    funkcja zwraca batch danych uczących\n",
    "    x to tensor o wymiarach (rozmiar batcha x (2 x długość kontekstu)\n",
    "    y ma wymiar rozmiar_batcha x 1\n",
    "    x i y zawierają indeksy tokenów i są typu torch.long\n",
    "    '''\n",
    "    rand_idx = np.random.randint(0, len(training_data), size=batch_size) # losujemy batch_size losowych indeksów\n",
    "    x = [training_data[idx][0] for idx in rand_idx]\n",
    "    y = [training_data[idx][1] for idx in rand_idx]\n",
    "    x = torch.tensor(x, dtype=torch.long)\n",
    "    y = torch.tensor(y, dtype=torch.long)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012286b1-267a-46c8-9496-39b31e2ff488",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_batch(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b18bca-cc25-4cad-8c74-a7e089334735",
   "metadata": {},
   "source": [
    "#### Sieć neuronowa\n",
    "* Zaimplementować model CBOWModel\n",
    "* *embedding_dim* będzie oznaczać rozmiar reprezentacji wektorowej, którą trenujemy\n",
    "* Model składa się z:\n",
    "    * warstwy Embedding o rozmiarze *vocab_size* x *embedding_dim* (*vocab_size* to liczba unikalnych tokenów w naszym tekście) - warstwa musi mieć nazwę embeddings\n",
    "    * oraz warstwy liniowej (Linear) o wymiarze *embedding_dim* x *vocab_size* (przewidujemy słowo spośród *vocab_size* słów)\n",
    "* W funkcji forward:\n",
    "    * Stosujemy warstwę embedding do danych wejściowych\n",
    "    * Wynikiem jest tensor o wymiarach *batch_size* x *n_words* x *embedding_dim* (*n_words* to liczba słów na wejściu (2 x *context_len*))\n",
    "    * Uśredniamy embeddingi dla wszystkich słów w obrębie przykładu. Wynikiem powinien być tensor o wymiarze *batch_size* x  *embedding_dim*\n",
    "    * Stosujemy warstwę liniową"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243fadcd-5067-42a9-9959-f17c79c73899",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__() \n",
    "        # TODO\n",
    "        \n",
    "    def forward(self, input_words): \n",
    "        # TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8d336-4bb1-4e9e-8df2-cd1de2eab8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eba7e5e-3047-4b7c-8b3b-32e0bb53b09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CBOWModel(vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a434f5-a6aa-40f7-8d7b-619e8a4ec961",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = model.embeddings(torch.tensor([[237, 2481, 164, 237], [237, 2481, 164, 237]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406d5012-8aac-4596-9177-5be604dc9ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert embs.shape == torch.Size([2, 4, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6c9e22-88b4-4bc0-8d45-0ebc4f9271a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model(torch.tensor([[237, 2481, 164, 237], [237, 2481, 164, 237]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c32b45e-53f9-4328-a04a-e1d8b7a42d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([2, vocab_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc2f132-1c82-4620-b4ff-4e19adfaa81a",
   "metadata": {},
   "source": [
    "### Uczenie modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74665b49-2ea1-4b91-9931-51365071a508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To jest wartość funkcji straty dla losowego klasyfikatora\n",
    "-np.log(1/vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84513236-2fee-4cbd-97ee-d015560466cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e250c98-b225-4263-8fec-6b8c7149ffab",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "n_steps = 2000\n",
    "batch_size = 4096\n",
    "model = CBOWModel(vocab_size, embedding_dim).to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.003) # AdamW = Adam + regularyzacja L2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6617d5a-0c2d-47f1-a784-122641b2789c",
   "metadata": {},
   "source": [
    "#### Zaimplementować pętlę uczenia\n",
    "* Podpowiedź - laboratorium 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172335bf-ef26-4445-a515-3c95de36b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for step in range(n_steps):\n",
    "     # TODO\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Krok {step}: Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f316fdc-caeb-4403-97d4-6006fb981967",
   "metadata": {},
   "source": [
    "#### Analiza uzyskanych wektorów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7d370c-080b-4803-b815-096e3d8e072d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_embedding(word):\n",
    "    '''\n",
    "    Funkcja zwraca wektor dla danego słowa\n",
    "    '''\n",
    "    word_idx = word_to_idx[word]\n",
    "    word_tensor = torch.tensor([word_idx], dtype=torch.long).to(device)\n",
    "    embedding = model.embeddings(word_tensor)\n",
    "    return embedding.cpu().detach().numpy()\n",
    "\n",
    "def find_similar(word, top_n=5):\n",
    "    '''\n",
    "    Funkcja zwraca top_n słów o najbliższych wektorach dla danego słowa\n",
    "    '''\n",
    "    embeddings = model.embeddings.weight.cpu().detach().numpy()\n",
    "    word_embedding = get_word_embedding(word)\n",
    "    similarities = cosine_similarity(word_embedding, embeddings)[0]\n",
    "    sorted_indices = similarities.argsort()[::-1]\n",
    "    similar_words = [idx_to_word[idx] for idx in sorted_indices[1:top_n+1]]\n",
    "    return similar_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de2f3fc-432c-41dd-a2c8-f0da3e0ae5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wektor dla przykładowego słowa\n",
    "word_embedding = get_word_embedding(\"network\")\n",
    "word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14480c1c-1b0d-4915-a6fb-b909cff5a2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kształt naszych embeddingów - macierz liczba słów x rozmiar wektora\n",
    "model.embeddings.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468753ce-a4bb-4462-9f6b-069dff715d9f",
   "metadata": {},
   "source": [
    "##### Zanjdziemy najbliższe słowa dla kilku przykładowych słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661ebd32-80f7-4aac-bf40-c5d0c6049288",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"1\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885c448-c07a-40e9-9a0b-f866ce7471b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar(\"1986\", 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20cd8bc0-ae0e-4991-b51e-837fefb748b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('king', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530a7b07-351e-4b04-859c-921b20cee92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('the', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff69ba02-b3c5-40ae-8d65-6a1928b4fbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('UNK', 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7d5db6-4028-4c21-93da-037104452b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "find_similar('poland', 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452c8c24-e496-4140-9107-09eaabc960db",
   "metadata": {},
   "source": [
    "#### Wizualizacja wybranych słów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0fd51c-2b31-4db8-a55a-312ce4e9f55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przykładowe słowa w czterech kategoriach\n",
    "words_to_vis = ['0', '00', '1', '10', '100', '110', '120', '150', '200', '250', '300', '500', '600', '700',\n",
    "                '800', '900'] +\\\n",
    "               ['1900', '1920s', '1950s', '1960s', '1970s', '1980s', '1990s', '2000', '10th', '11th', '12th',\n",
    "                '19th', '20th', '21st'] +\\\n",
    "               ['school', 'teacher', 'students', 'class', 'university', 'college', 'education', 'degree',\n",
    "                'knowledge'] +\\\n",
    "               ['america', 'argentina', 'australia', 'brazil', 'canada', 'china', 'egypt', 'england',\n",
    "                'france', 'germany', 'india', 'ireland', 'israel', 'italy', 'japan', 'mexico', 'norway',\n",
    "                'palestine', 'poland', 'portugal', 'russia', 'somalia', 'spain', 'vietnam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad56ecb-4778-4184-8e0e-dba5e9d86d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyznaczamy wektory\n",
    "embs = np.array([get_word_embedding(word)[0] for word in words_to_vis])\n",
    "embs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d7713d-1b36-40d5-95e7-be787c24cd9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Za pomocą algorytmu T-SNE znajdujemy znaurzenia wektorów w przestrzeń 2d\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embs_2d = tsne.fit_transform(embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd352bd-5ccb-4799-9bd8-cfe8a46dde6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "embs_2d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f72fe30-09f6-454f-aba0-83dad6bc2b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(embs_2d[:, 0], embs_2d[:, 1], alpha=0.6)\n",
    "for i in range(len(words_to_vis)):\n",
    "    plt.annotate(words_to_vis[i], (embs_2d[:, 0][i], embs_2d[:, 1][i]),\n",
    "                 textcoords=\"offset points\", xytext=(5, 5), ha='center', fontsize=7, color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea2920f-6bf2-4df8-b361-d3cd57d21153",
   "metadata": {},
   "source": [
    "#### Wnioski\n",
    "1. Skomentuj powyższy wykres\n",
    "2. Znajdź dwa słowa, dla których słowa o najbliższych wektorach wyglądają rozsądnie\n",
    "3. Znajdź dwa słowa, dla których słowa o najbliższych wektorach wyglądają zaskakująco lub bez sensu\n",
    "4. Jaka może być przyczyna (3)?\n",
    "5. [dla chętnych] Lepsze parametry uczenia (rozmiar wektorów, współczynnik uczenia, optymalizator, rozmiar batcha, ...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec4aa9cf-a678-424f-b631-f0802d111aa7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dydaktyka_LLM",
   "language": "python",
   "name": "dydaktyka_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
