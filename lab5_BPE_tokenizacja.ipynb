{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201d5d8f-0eb6-456b-a6fd-12291fe26bc1",
   "metadata": {},
   "source": [
    "## LAB 5 Jak działa GPT\n",
    "### Tokenizacja z wykorzystaniem algorytmu BPE\n",
    "\n",
    "Celem laboratorium jest:\n",
    "\n",
    "* implementacja algorytmu BPE\n",
    "* zrozumienie koncepcji tokenizacji\n",
    "\n",
    "#### Źródła:\n",
    "* https://github.com/karpathy/minbpe\n",
    "* https://youtu.be/zduSFxRajkE?si=pHViKDX4I6yrCqf6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3269f9a2-9abf-4d04-8407-f315e4a86878",
   "metadata": {},
   "source": [
    "Imię i nazwisko: .....\n",
    "\n",
    "Punktacja:\n",
    "* Prawidłowa implementacja 6 pkt.\n",
    "* Wnioski 2 pkt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc99ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c15eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jako tekst ponownie wykorzystamy HPMOR\n",
    "# Eliezer Yudkowsky, Harry Potter and the Methods of Rationality https://hpmor.com/\n",
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/hpmor_part.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ca64bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e298d6-7af2-444c-be0c-50ae65ddef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamieniamy tekst na bajty z kodowania utf8\n",
    "text_bytes = list(text.encode('utf8'))\n",
    "print(text_bytes[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db43085-a11e-4966-8b9a-5eb4bcb6c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy początkowy słownik - ponieważ pracujemy na bajtach to mają one wartości od 0 do 255\n",
    "vocab = {i: bytes([i]) for i in range(256)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcef422-34a8-477b-9ba7-b65e64ec23d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9caaf-2ba9-40c3-908b-900f48eb8914",
   "metadata": {},
   "source": [
    "#### Algorytm BPE\n",
    "Będziemy implementować kroki algorytmu BPE:\n",
    "1. znaleźć najczęstszą parę tokenów\n",
    "2. zastąpić ją nowym tokenem\n",
    "3. powtórzyć kroki 1 i 2, aż do osiągnięcia zadanego limitu liczby połączeń (n_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905c6ca",
   "metadata": {},
   "source": [
    "##### Zaimplementuj funkcję znajdującą najczęstszą parę tokenów\n",
    "1. stwórz listę **par kolejnych** tokenów\n",
    "2. zlicz wystąpienia każdej pary (wskazówka: Counter)\n",
    "3. znajdź parę z największą liczbą wystąpień (wskazówka: most_common i końcówka notebooka z wykładu)\n",
    "4. zwróć parę\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd365db-b7cb-45f9-9afb-3757bcd89361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_pair(tokens_list):\n",
    "    #TODO\n",
    "    #TODO\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2797b9d-c72d-4dbe-bc3b-6dee3a20c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na początku nasza lista tokenów to lista bajtów z tekstu\n",
    "tokens_list = list(text_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ed85de-3dde-4b82-bcb6-a59fbf96e6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_freq_pair = find_most_frequent_pair(tokens_list)\n",
    "most_freq_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6fb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert most_freq_pair == (101, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b320de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# znaki tworzące najczęstszą parę tokenów\n",
    "print(vocab[most_freq_pair[0]])\n",
    "print(vocab[most_freq_pair[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e88941-6404-40dc-bc03-e0e6d69d4f3b",
   "metadata": {},
   "source": [
    "#### Napisz funkcję, która zastępuje wybraną parę tokenów nowym tokenem\n",
    "*Uwaga: to jest miejsce gdzie jest najwięcej pracy i najłatwiej się pomylić, dalej będzie łatwiej*\n",
    "* uwaga: jeśli zastępujemy to trzeba w przetwarzaniu ominąć następny token\n",
    "    * tzn. w poniższym przykładzie zastępując (1,2) przez 5, to przetwarzanie 2 pomijamy\n",
    "* uwaga: proszę nie gubić ostatniego tokenu\n",
    "\n",
    "Przykład: replace([1,2,3,4], (1,2), 5) -> [5,3,4]\n",
    "\n",
    "Kroki:\n",
    "1. stwórz nową listę tokenów\n",
    "2. przejdź pętlą for przez listę tokenów\n",
    "    * sprawdzaj parę token i następny token\n",
    "    * jeśli napotkasz wybraną parę tokenów, to dodaj nowy token do nowej listy i pomiń następny token\n",
    "    * jeśli nie napotkasz pary tokenów, to dodaj aktualny token do nowej listy\n",
    "3. jeśli nie został pominięty ostatni token, to dodaj go do nowej listy\n",
    "4. zwróć nową listę tokenów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2ecae-54ce-47eb-851d-857aef6b49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(tokens_list, pair, new_token):\n",
    "    new_tokens_list = []\n",
    "    # TODO\n",
    "    return new_tokens_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a229bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace([1, 2, 3, 4], (1,2), 5) == [5, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9356dc2-be06-4ba1-b209-b4157a5324ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_idx = 256 # pierwszy wolny indeks\n",
    "new_tokens_list = replace(tokens_list, most_freq_pair, next_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b114c4f-c3e9-4f72-b59c-7a5c6c27a1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokeny przed zamianą (20 pierwszych)\n",
    "print(tokens_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e9e9b7-c988-4bd1-bafc-0b5d3226b82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokeny po połączeniu (20 pierwszych)\n",
    "print(new_tokens_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28450f",
   "metadata": {},
   "source": [
    "#### Zaimplementujemy 50 połączeń\n",
    "\n",
    "Potrzebujemy słownika merges, który będzie przechowywał mapowanie par tokenów na nowe tokeny.\n",
    "Kluczami są pary tokenów, a wartością nowy token.\n",
    "Przykład:\n",
    "{(101, 32): 256,\n",
    " (32, 116): 257,\n",
    " (116, 32): 258}\n",
    "\n",
    " Słownik ten będzie nam potrzebny do kodowania i dekodowania tekstu.\n",
    "\n",
    " Kroki:\n",
    "1. stwórz pusty słownik merges\n",
    "2. przejdź pętlą while, aż do osiągnięcia zadanego limitu liczby połączeń (n_merges) (warto sprawdzać czy lista tokenów ma więcej niż 1 token)\n",
    "    * znajdź najczęstszą parę tokenów\n",
    "    * zastąp ją nowym tokenem\n",
    "    * dodaj parę do słownika merges\n",
    "    * zwiększ indeks nowego tokenu\n",
    "    * zwiększ licznik połączeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99666e4d-2980-496c-aeb5-09afff269430",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_merges = 50\n",
    "next_token_idx = 256\n",
    "tokens_list = list(text_bytes)\n",
    "merges = {}\n",
    "merges_counter = 0\n",
    "while merges_counter < n_merges and len(tokens_list) > 1:\n",
    "    most_freq_pair = # TODO\n",
    "    tokens_list =  # TODO\n",
    "    merges[most_freq_pair] =  # TODO\n",
    "    # TODO\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f146f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens_list) == 10601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd4c094-5905-4525-8066-3e4660a628ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokens_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886698a5-fc23-47b3-9de8-1b9dd8624b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7284cf-bb78-4358-8cad-ed1d865ebe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aktualizaujemy nasz słownik o nowe tokeny\n",
    "for (t1, t2), tm in merges.items():\n",
    "    vocab[tm] = vocab[t1] + vocab[t2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af57c0d5-413d-4761-b5c4-54f39cde9372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przejrzyj tokeny powyżej 256 - jakie są?\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b32de0-544c-41d5-9df2-018df69c08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamiana tekstu na tokeny\n",
    "def encode(text):\n",
    "    text_bytes = list(text.encode('utf8'))\n",
    "    tokens_list = list(text_bytes)\n",
    "    for pair, token in merges.items():\n",
    "        tokens_list = replace(tokens_list, pair, token)\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaf109f-bcdb-4ce8-a7e6-04fe0cc54c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamiana tokenów na tekst\n",
    "def decode(tokens_list):\n",
    "    text_bytes = [vocab[t] for t in tokens_list]\n",
    "    merged_bytes = b''.join(text_bytes)\n",
    "    return merged_bytes.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3505ebee-9e84-4cba-81b7-00a51a575900",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_list = encode(text)\n",
    "print(tokens_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802bdde2-7f8f-49f6-8e8b-a20163a7cba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# powinniśmy otrzymać z powrotem tekst\n",
    "decode(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65ad88e5-7b71-4e5e-b93f-506dd8ea8354",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('długość tekstu:', len(text))\n",
    "print('długość tekstu w bajtach:', len(list(text.encode('utf8'))))\n",
    "print('liczba tokenów po tokenizacji:', len(tokens_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee7246-8234-4104-b79a-51faf22a85b4",
   "metadata": {},
   "source": [
    "### Klasa tokenizer\n",
    "* korzystając z kodu powyżej zaimplementuj klasę Tokenizer\n",
    "* zaimplementuj funkcje:\n",
    "    * train\n",
    "    * encode\n",
    "    * decode\n",
    "    * find_most_frequent_pair\n",
    "    * replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8514678-214a-40af-82a0-03466c5e3105",
   "metadata": {},
   "source": [
    "#### Uwaga\n",
    "* Przed testmi implementacji warto zrestartować kernel, żeby być pewnym, że nie korzysta się ze starych zmiennych globalnych\n",
    "* Potem trzeba ponownie wywołać komórki z importem bibliotek i wczytaniem tekstu (dwie pierwsze komórki z kodem od góry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f553a-5140-466b-8fbb-d8aa8fc39a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {} # słownik tokenów - numer tokenu -> tekst tokenu\n",
    "        self.merges = {} # słownik połączeń - para tokenów -> nowy token\n",
    "        self.next_token_idx = 0 # indeks nowego tokenu (pierwszy wolny indeks)\n",
    "\n",
    "    def train(self, text, n_merges=100): # n_merges to liczba połączeń\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "        self.next_token_idx = 256\n",
    "        tokens_list = list(text.encode('utf8'))\n",
    "        self.merges = {}\n",
    "        # wykonaj zadaną liczbę połączeń, aktualizując listę tokenów, słownik merges i indeks nowego tokenu\n",
    "        #TODO\n",
    "\n",
    "        # aktualizacja słownika tokenów\n",
    "        for (t1, t2), tm in self.merges.items():\n",
    "            self.vocab[tm] = self.vocab[t1] + self.vocab[t2]\n",
    "\n",
    "    def encode(self, text):\n",
    "        # zamiana tekstu na listę tokenów\n",
    "        # TODO\n",
    "        return tokens_list\n",
    "\n",
    "    def decode(self, tokens_list):\n",
    "        # zamiana listy tokenów na tekst\n",
    "        # TODO\n",
    "        return #TODO\n",
    "\n",
    "    def find_most_frequent_pair(self, tokens_list):\n",
    "        # znajdź najczęstszą parę tokenów\n",
    "        # TODO\n",
    "        return #TODO\n",
    "\n",
    "    def replace(self, tokens_list, pair, new_token):\n",
    "        # zamiana pary tokenów na nowy token\n",
    "        # funkcja zwraca nową listę tokenów\n",
    "        new_tokens_list = []\n",
    "        # TODO\n",
    "        return new_tokens_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4ac05-96ac-43ed-9d73-b7a3e00899a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295d6cdc-0390-4541-9e10-40415af84352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uczymy tokenizer\n",
    "tokenizer.train(text, n_merges=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3960288-0df5-45fb-bb7d-4b65116d0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78948c92-2073-4cde-818c-cf7bc1ef91aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c1823-f2c1-479c-8ed0-17ffc9f74882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sprawdzamy czy zamiana działa\n",
    "tokenizer.replace([1,2,3,4], (1,2), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be0a59bd-7aab-4d5d-a6cb-fd732062ff46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizujemy fragment tekstu\n",
    "tokens_list = tokenizer.encode(text[:70])\n",
    "print(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4026edb-3e5e-47a3-bbcc-01fd5922e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4807bf5-469c-4ccd-aaad-8312459c4b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i dekodujemy fragment tekstu (powinniśmy dostać z powrotem ten sam tekst)\n",
    "tokenizer.decode(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b3481e-8ce8-4814-92ae-60627a5796d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(text))\n",
    "print(len(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7f9bc-d146-4410-8545-bd4687d5b24d",
   "metadata": {},
   "source": [
    "#### Uczenie na dłuższym korpusie tekstu po polsku i po angielsku\n",
    "* Wykorzystamy polski przekład Tajemnicy Baskervilleow\n",
    "    * Tekst pobrany z https://www.gutenberg.org/\n",
    "* Oraz dłuższy fragment (10 rozdziałów) HPMOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42976961-718e-4ca4-ad6e-601aab1b079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/hpmor_chapters_1-10.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text_eng = response.text\n",
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/TajemnicaBaskervilleow.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text_pl = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8178d212-7c5b-4e17-af60-8968ddf57758",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_pl[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f42cb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_eng[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ee41e3-919e-4107-8a47-7002b87baeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('długość polskiego tekstu:', len(text_pl))\n",
    "print('długość angielskiego tekstu:', len(text_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78123f8e-56a1-4bd6-9454-d103b49012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pl = Tokenizer()\n",
    "tokenizer_pl.train(text_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e1a15-cdbf-4c83-aec9-47aa429409e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in tokenizer_pl.vocab.items():\n",
    "    if k > 256:\n",
    "        print(k, v.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e804ea6-07c8-439a-b48a-dd234c63c8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer_pl.encode(text_pl)\n",
    "print('długość polskiego tekstu:', len(text_pl))\n",
    "print('długość polskiego tekstu w bajtach:', len(list(text_pl.encode('utf8'))))\n",
    "print('liczba tokenów po tokenizacji:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb35cb7-09a1-4ff5-a0ac-5474509ed026",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens)==148913"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5317c",
   "metadata": {},
   "source": [
    "#### Powtórz powyższe dla tekstu angielskiego\n",
    "* utwórz obiekt tokenizer_eng\n",
    "* naucz tokenizer_eng na tekście angielskim\n",
    "* wypisz słownik tokenów\n",
    "* tokenizuj tekst angielski\n",
    "* wypisz długość tekstu w bajtach i liczbę tokenów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f726e49d-b991-4c4a-a192-a1d0f09d1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utworzyć obiekt tokenizer_eng i nauczyć go na tekście angielskim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7355b947-fc6f-4e34-9d7a-dddec02d75d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wyświetlić słownik tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd5a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Tokenizować tekst angielski\n",
    "# TODO: Wyświetlić długość tekstu w bajtach i liczbę tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb55cd-0d92-4b2a-9f98-691afbddb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens)==151373"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52b41f",
   "metadata": {},
   "source": [
    "#### Wykonaj tokenizację tekstu polskiego z wykorzystaniem tokenizatora nauczonego na tekście angielskim\n",
    "* wypisz długość tekstu w tokenach po tokenizacji angielskiego tokenizera\n",
    "* wypisz długość tekstu w tokenach po tokenizacji polskiego tokenizera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe214937-bce6-4049-9b8d-b5d3d0ae60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wykonać tokenizację tekstu polskiego z wykorzystaniem tokenizatora nauczonego na tekście angielskim\n",
    "# TODO: Wyświetlić długość tekstu w tokenach po tokenizacji angielskiego tokenizera\n",
    "# TODO: Wyświetlić długość tekstu w tokenach po tokenizacji polskiego tokenizera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40764162-2a62-4128-a27d-a16131f3a18c",
   "metadata": {},
   "source": [
    "### Pytania\n",
    "1. Jakie tokeny uzyskaliśmy dla uczenia na tekście angielskim? A jakie dla uczenia na tekście polskim? Czemu są różne?\n",
    "2. Dla tekstu angielskiego liczba znaków i liczba bajtów jest taka sama. Dlaczego dla tekstu polskiego liczba bajtów jest większa?\n",
    "3. Na końcu wykorzystujemy tokenizator nauczony na tekście angielskim do tokenizacji tekstu polskiego. Jak ocenisz wynik? Czemu liczba tokenów różni się od liczby tokenów uzyskanej przy tokenizacji z wykorzystaniem tokenizatora nauczonego na tekście polskim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b324d-181d-4dbc-9161-d1e3ea7f8a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
