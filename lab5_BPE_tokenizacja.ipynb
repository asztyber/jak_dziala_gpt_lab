{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "201d5d8f-0eb6-456b-a6fd-12291fe26bc1",
   "metadata": {},
   "source": [
    "## LAB 5 Jak działa GPT\n",
    "### Tokenizacja z wykorzystaniem algorytmu BPE\n",
    "\n",
    "Celem laboratorium jest:\n",
    "\n",
    "* implementacja algorytmu BPE\n",
    "* zrozumienie koncepcji tokenizacji\n",
    "\n",
    "#### Źródła:\n",
    "* https://github.com/karpathy/minbpe\n",
    "* https://youtu.be/zduSFxRajkE?si=pHViKDX4I6yrCqf6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdc99ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3c15eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jako tekst ponownie wykorzystamy HPMOR\n",
    "# Eliezer Yudkowsky, Harry Potter and the Methods of Rationality https://hpmor.com/\n",
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/hpmor_part.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86ca64bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beneath the moonlight glints a tiny fragment of silver, a fraction of a line... (black robes, fallin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5e298d6-7af2-444c-be0c-50ae65ddef27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 101, 110, 101, 97, 116, 104, 32, 116, 104, 101, 32, 109, 111, 111, 110, 108, 105, 103, 104, 116, 32, 103, 108, 105, 110, 116, 115, 32, 97, 32, 116, 105, 110, 121, 32, 102, 114, 97, 103, 109, 101, 110, 116, 32, 111, 102, 32, 115, 105]\n"
     ]
    }
   ],
   "source": [
    "# zamieniamy tekst na bajty z kodowania utf8\n",
    "text_bytes = list(text.encode('utf8'))\n",
    "print(text_bytes[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0db43085-a11e-4966-8b9a-5eb4bcb6c822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy początkowy słownik - ponieważ pracujemy na bajtach to mają one wartości od 0 do 255\n",
    "vocab = {i: bytes([i]) for i in range(256)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fcef422-34a8-477b-9ba7-b65e64ec23d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff'}\n"
     ]
    }
   ],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad9caaf-2ba9-40c3-908b-900f48eb8914",
   "metadata": {},
   "source": [
    "#### Algorytm BPE\n",
    "Będziemy implementować kroki algorytmu BPE:\n",
    "1. znaleźć najczęstszą parę tokenów\n",
    "2. zastąpić ją nowym tokenem\n",
    "3. powtórzyć kroki 1 i 2, aż do osiągnięcia zadanego limitu liczby połączeń (n_merges)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0905c6ca",
   "metadata": {},
   "source": [
    "##### Zaimplementuj funkcję znajdującą najczęstszą parę tokenów\n",
    "1. stwórz listę **par** tokenów\n",
    "2. zlicz wystąpienia każdej pary (wskazówka: Counter)\n",
    "3. znajdź parę z największą liczbą wystąpień (wskazówka: most_common i końcówka notebooka z wykładu)\n",
    "4. zwróć parę\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bdd365db-b7cb-45f9-9afb-3757bcd89361",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_frequent_pair(tokens_list):\n",
    "    #TODO\n",
    "    #TODO\n",
    "    #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2797b9d-c72d-4dbe-bc3b-6dee3a20c17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# na początku nasza lista tokenów to lista bajtów z tekstu\n",
    "tokens_list = list(text_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1ed85de-3dde-4b82-bcb6-a59fbf96e6ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(101, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq_pair = find_most_frequent_pair(tokens_list)\n",
    "most_freq_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb6fb869",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert most_freq_pair == (101, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31b320de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'e'\n",
      "b' '\n"
     ]
    }
   ],
   "source": [
    "# znaki tworzące najczęstszą parę tokenów\n",
    "print(vocab[most_freq_pair[0]])\n",
    "print(vocab[most_freq_pair[1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e88941-6404-40dc-bc03-e0e6d69d4f3b",
   "metadata": {},
   "source": [
    "#### Napisz funkcję, która zastępuje wybraną parę tokenów nowym tokenem\n",
    "*Uwaga: to jest miejsce gdzie jest najwięcej pracy i najłatwiej się pomylić, dalej będzie łatwiej*\n",
    "* uwaga: jeśli zastępujemy to trzeba w przetwarzaniu ominąć następny token\n",
    "    * tzn. w poniższym przykładzie zastępując (1,2) przez 5, to w przetwarzanie 2 pomijamy\n",
    "* uwaga: proszę nie gubić ostatniego tokenu\n",
    "\n",
    "Przykład: replace([1,2,3,4], (1,2), 5) -> [5,3,4]\n",
    "\n",
    "Kroki:\n",
    "1. stwórz nową listę tokenów\n",
    "2. przejdź pętlą for przez listę tokenów\n",
    "    * sprawdzaj parę token i następny token\n",
    "    * jeśli napotkasz wybraną parę tokenów, to dodaj nowy token do nowej listy i pomiń następny token\n",
    "    * jeśli nie napotkasz pary tokenów, to dodaj aktualny token do nowej listy\n",
    "3. jeśli nie został pominięty ostatni token, to dodaj go do nowej listy\n",
    "4. zwróć nową listę tokenów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "96c2ecae-54ce-47eb-851d-857aef6b49ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(tokens_list, pair, new_token):\n",
    "    new_tokens_list = []\n",
    "    # TODO\n",
    "    return new_tokens_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a229bc16",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert replace([1, 2, 3, 4], (1,2), 5) == [5, 3, 4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9356dc2-be06-4ba1-b209-b4157a5324ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_token_idx = 256 # pierwszy wolny indeks\n",
    "new_tokens_list = replace(tokens_list, most_freq_pair, next_token_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b114c4f-c3e9-4f72-b59c-7a5c6c27a1a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 101, 110, 101, 97, 116, 104, 32, 116, 104, 101, 32, 109, 111, 111, 110, 108, 105, 103, 104]\n"
     ]
    }
   ],
   "source": [
    "# tokeny przed zamianą (20 pierwszych)\n",
    "print(tokens_list[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8e9e9b7-c988-4bd1-bafc-0b5d3226b82e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 101, 110, 101, 97, 116, 104, 32, 116, 104, 256, 109, 111, 111, 110, 108, 105, 103, 104, 116]\n"
     ]
    }
   ],
   "source": [
    "# tokeny po połączeniu (20 pierwszych)\n",
    "print(new_tokens_list[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28450f",
   "metadata": {},
   "source": [
    "#### Zaimplementujemy 50 połączeń\n",
    "\n",
    "Potrzebujemy słownika merges, który będzie przechowywał mapowanie par tokenów na nowe tokeny.\n",
    "Kluczami są pary tokenów, a wartością nowy token.\n",
    "Przykład:\n",
    "{(101, 32): 256,\n",
    " (32, 116): 257,\n",
    " (116, 32): 258}\n",
    "\n",
    " Słownik ten będzie nam potrzebny do kodowania i dekodowania tekstu.\n",
    "\n",
    " Kroki:\n",
    "1. stwórz pusty słownik merges\n",
    "2. przejdź pętlą while, aż do osiągnięcia zadanego limitu liczby połączeń (n_merges) (warto sprawdzać czy lista tokenów ma więcej niż 1 token)\n",
    "    * znajdź najczęstszą parę tokenów\n",
    "    * zastąp ją nowym tokenem\n",
    "    * dodaj parę do słownika merges\n",
    "    * zwiększ indeks nowego tokenu\n",
    "    * zwiększ licznik połączeń"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "99666e4d-2980-496c-aeb5-09afff269430",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_merges = 50\n",
    "next_token_idx = 256\n",
    "tokens_list = list(text_bytes)\n",
    "merges = {}\n",
    "merges_counter = 0\n",
    "while merges_counter < n_merges and len(tokens_list) > 1:\n",
    "    most_freq_pair = # TODO\n",
    "    tokens_list =  # TODO\n",
    "    merges[most_freq_pair] =  # TODO\n",
    "    # TODO\n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f146f52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens_list) == 10601"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6dd4c094-5905-4525-8066-3e4660a628ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 268, 101, 97, 273, 276, 109, 111, 274, 108, 105, 103, 104, 258, 103, 108, 260, 116, 261, 97, 257, 260, 270, 102, 114, 97, 103, 109, 268, 258, 305, 115, 105, 108, 118, 262, 266, 284, 102, 114, 97, 99, 116, 105, 274, 32, 305, 284, 108, 260, 101, 46, 46, 265, 40, 98, 108, 97, 99, 107, 32, 114, 111, 98, 279, 266, 102, 294, 108, 271, 41, 32, 46, 46, 46, 98, 108, 111, 111, 259, 115, 112, 105, 108, 108, 261, 267, 258, 260, 32, 108, 299, 114, 279, 266, 286, 115, 298, 101, 274]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "886698a5-fc23-47b3-9de8-1b9dd8624b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(101, 32): 256,\n",
       " (32, 116): 257,\n",
       " (116, 32): 258,\n",
       " (100, 32): 259,\n",
       " (105, 110): 260,\n",
       " (115, 32): 261,\n",
       " (101, 114): 262,\n",
       " (257, 104): 263,\n",
       " (97, 110): 264,\n",
       " (46, 32): 265,\n",
       " (44, 32): 266,\n",
       " (111, 117): 267,\n",
       " (101, 110): 268,\n",
       " (97, 114): 269,\n",
       " (121, 32): 270,\n",
       " (260, 103): 271,\n",
       " (111, 32): 272,\n",
       " (116, 104): 273,\n",
       " (111, 110): 274,\n",
       " (111, 114): 275,\n",
       " (263, 256): 276,\n",
       " (262, 32): 277,\n",
       " (101, 259): 278,\n",
       " (101, 115): 279,\n",
       " (104, 105): 280,\n",
       " (104, 97): 281,\n",
       " (101, 108): 282,\n",
       " (102, 32): 283,\n",
       " (97, 32): 284,\n",
       " (105, 99): 285,\n",
       " (264, 259): 286,\n",
       " (119, 97): 287,\n",
       " (271, 32): 288,\n",
       " (34, 32): 289,\n",
       " (115, 116): 290,\n",
       " (257, 272): 291,\n",
       " (111, 119): 292,\n",
       " (101, 116): 293,\n",
       " (97, 108): 294,\n",
       " (73, 32): 295,\n",
       " (97, 258): 296,\n",
       " (121, 267): 297,\n",
       " (111, 109): 298,\n",
       " (105, 116): 299,\n",
       " (101, 100): 300,\n",
       " (269, 114): 301,\n",
       " (104, 256): 302,\n",
       " (108, 270): 303,\n",
       " (72, 301): 304,\n",
       " (111, 283): 305}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b7284cf-bb78-4358-8cad-ed1d865ebe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# aktualizaujemy nasz słownik o nowe tokeny\n",
    "for (t1, t2), tm in merges.items():\n",
    "    vocab[tm] = vocab[t1] + vocab[t2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af57c0d5-413d-4761-b5c4-54f39cde9372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'e ', 257: b' t', 258: b't ', 259: b'd ', 260: b'in', 261: b's ', 262: b'er', 263: b' th', 264: b'an', 265: b'. ', 266: b', ', 267: b'ou', 268: b'en', 269: b'ar', 270: b'y ', 271: b'ing', 272: b'o ', 273: b'th', 274: b'on', 275: b'or', 276: b' the ', 277: b'er ', 278: b'ed ', 279: b'es', 280: b'hi', 281: b'ha', 282: b'el', 283: b'f ', 284: b'a ', 285: b'ic', 286: b'and ', 287: b'wa', 288: b'ing ', 289: b'\" ', 290: b'st', 291: b' to ', 292: b'ow', 293: b'et', 294: b'al', 295: b'I ', 296: b'at ', 297: b'you', 298: b'om', 299: b'it', 300: b'ed', 301: b'arr', 302: b'he ', 303: b'ly ', 304: b'Harr', 305: b'of '}\n"
     ]
    }
   ],
   "source": [
    "# przejrzyj tokeny powyżej 256 - jakie są?\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6b32de0-544c-41d5-9df2-018df69c08c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamiana tekstu na tokeny\n",
    "def encode(text):\n",
    "    text_bytes = list(text.encode('utf8'))\n",
    "    tokens_list = list(text_bytes)\n",
    "    for pair, token in merges.items():\n",
    "        tokens_list = replace(tokens_list, pair, token)\n",
    "    return tokens_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dbaf109f-bcdb-4ce8-a7e6-04fe0cc54c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamiana tokenów na tekst\n",
    "def decode(tokens_list):\n",
    "    text_bytes = [vocab[t] for t in tokens_list]\n",
    "    merged_bytes = b''.join(text_bytes)\n",
    "    return merged_bytes.decode('utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3505ebee-9e84-4cba-81b7-00a51a575900",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'encode' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens_list \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m(text[:\u001b[38;5;241m500\u001b[39m])\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(tokens_list[:\u001b[38;5;241m100\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'encode' is not defined"
     ]
    }
   ],
   "source": [
    "tokens_list = encode(text)\n",
    "print(tokens_list[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "802bdde2-7f8f-49f6-8e8b-a20163a7cba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beneath the moonlight glints a tiny fragment of silver, a fraction of a line... (black robes, falling) ...blood spills out in litres, and someone screams a word.  Every inch of wall space is covered by a bookcase. Each bookcase has six shelves, going almost to the ceiling. Some bookshelves are stacked to the brim with hardback books: science, maths, history, and everything else. Other shelves have two layers of paperback science fiction, with the back layer of books propped up on old tissue boxes or lengths of wood, so that you can see the back layer of books above the books in front. And it still isn\\'t enough. Books are overflowing onto the tables and the sofas and making little heaps under the windows. This is the living-room of the house occupied by the eminent Professor Michael Verres-Evans, and his wife, Mrs. Petunia Evans-Verres, and their adopted son, Harry James Potter-Evans-Verres. There is a letter lying on the living-room table, and an unstamped envelope of yellowish parchment, addressed to Mr. H. Potter in emerald-green ink. The Professor and his wife are speaking sharply at each other, but they are not shouting. The Professor considers shouting to be uncivilised. \"You\\'re joking,\" Michael said to Petunia. His tone indicated that he was very much afraid that she was serious. \"My sister was a witch,\" Petunia repeated. She looked frightened, but stood her ground. \"Her husband was a wizard.\" \"This is absurd!\" Michael said sharply. \"They were at our wedding - they visited for Christmas -\" \"I told them you weren\\'t to know,\" Petunia whispered. \"But it\\'s true. I\\'ve seen things -\" The Professor rolled his eyes. \"Dear, I understand that you\\'re not familiar with the sceptical literature. You may not realise how easy it is for a trained magician to fake the seemingly impossible. Remember how I taught Harry to bend spoons? If it seemed like they could always guess what you were thinking, that\\'s called cold reading -\" \"It wasn\\'t bending spoons -\" \"What was it, then?\" Petunia bit her lip. \"I can\\'t just tell you. You\\'ll think I\\'m -\" She swallowed. \"Listen. Michael. I wasn\\'t - always like this -\" She gestured at herself, as though to indicate her lithe form. \"Lily did this. Because I - because I begged her. For years, I begged her. Lily had always been prettier than me, and I\\'d... been mean to her, because of that, and then she got magic, can you imagine how I felt? And I begged her to use some of that magic on me so that I could be pretty too, even if I couldn\\'t have her magic, at least I could be pretty.\" Tears were gathering in Petunia\\'s eyes. \"And Lily would tell me no, and make up the most ridiculous excuses, like the world would end if she were nice to her sister, or a centaur told her not to - the most ridiculous things, and I hated her for it. And when I had just graduated from university, I was going out with this boy, Vernon Dursley, he was fat and he was the only boy who would talk to me. And he said he wanted children, and that his first son would be named Dudley. And I thought to myself, what kind of parent names their child Dudley Dursley? It was like I saw my whole future life stretching out in front of me, and I couldn\\'t stand it. And I wrote to my sister and told her that if she didn\\'t help me I\\'d rather just -\" Petunia stopped. \"Anyway,\" Petunia said, her voice small, \"she gave in. She told me it was dangerous, and I said I didn\\'t care any more, and I drank this potion and I was sick for weeks, but when I got better my skin cleared up and I finally filled out and... I was beautiful, people were nice to me,\" her voice broke, \"and after that I couldn\\'t hate my sister any more, especially when I learned what her magic brought her in the end -\" \"Darling,\" Michael said gently, \"you got sick, you gained some weight while resting in bed, and your skin cleared up on its own. Or being sick made you change your diet -\" \"She was a witch,\" Petunia repeated. \"I saw it.\" \"Petunia,\" Michael said. The annoyance was creeping into his voice. \"You know that can\\'t be true. Do I really have to explain why?\" Petunia wrung her hands. She seemed to be on the verge of tears. \"My love, I know I can\\'t win arguments with you, but please, you have to trust me on this -\" \"Dad! Mum!\" The two of them stopped and looked at Harry as though they\\'d forgotten there was a third person in the room. Harry took a deep breath. \"Mum, your parents didn\\'t have magic, did they?\" \"No,\" Petunia said, looking puzzled. \"Then no one in your family knew about magic when Lily got her letter. How did they get convinced?\" \"Ah...\" Petunia said. \"They didn\\'t just send a letter. They sent a professor from Hogwarts. He -\" Petunia\\'s eyes flicked to Michael. \"He showed us some magic.\" \"Then you don\\'t have to fight over this,\" Harry said firmly. Hoping against hope that this time, just this once, they would listen to him. \"If it\\'s true, we can just get a Hogwarts professor here and see the magic for ourselves, and Dad will admit that it\\'s true. And if not, then Mum will admit that it\\'s false. That\\'s what the experimental method is for, so that we don\\'t have to resolve things just by arguing.\" The Professor turned and looked down at him, dismissive as usual. \"Oh, come now, Harry. Really, magic? I thought you\\'d know better than to take this seriously, son, even if you\\'re only ten. Magic is just about the most unscientific thing there is!\" Harry\\'s mouth twisted bitterly. He was treated well, probably better than most genetic fathers treated their own children. Harry had been sent to the best primary schools - and when that didn\\'t work out, he was provided with tutors from the endless pool of starving students. Always Harry had been encouraged to study whatever caught his attention, bought all the books that caught his fancy, sponsored in whatever maths or science competitions he entered. He was given anything reasonable that he wanted, except, maybe, the slightest shred of respect. A Doctor teaching biochemistry at Oxford could hardly be expected to listen to the advice of a little boy. You would listen to Show Interest, of course; that\\'s what a Good Parent would do, and so, if you conceived of yourself as a Good Parent, you would do it. But take a ten-year-old seriously? Hardly. Sometimes Harry wanted to scream at his father. \"Mum,\" Harry said. \"If you want to win this argument with Dad, look in chapter two of the first book of the Feynman Lectures on Physics. There\\'s a quote there about how philosophers say a great deal about what science absolutely requires, and it is all wrong, because the only rule in science is that the final arbiter is observation - that you just have to look at the world and report what you see. Um... off the top of my head I can\\'t think of where to find something about how it\\'s an ideal of science to settle things by experiment instead of arguments -\" His mother looked down at him and smiled. \"Thank you, Harry. But -\" her head rose back up to stare at her husband. \"I don\\'t want to win an argument with your father. I want my husband to, to listen to his wife who loves him, and trust her just this once -\" Harry closed his eyes briefly. Hopeless. Both of his parents were just hopeless. Now his parents were getting into one of those arguments again, one where his mother tried to make his father feel guilty, and his father tried to make his mother feel stupid. \"I\\'m going to go to my room,\" Harry announced. His voice trembled a little. \"Please try not to fight too much about this, Mum, Dad, we\\'ll know soon enough how it comes out, right?\" \"Of course, Harry,\" said his father, and his mother gave him a reassuring kiss, and then they went on fighting while Harry climbed the stairs to his bedroom. He shut the door behind him and tried to think. The funny thing was, he should have agreed with Dad. No one had ever seen any evidence of magic, and according to Mum, there was a whole magical world out there. How could anyone keep something like that a secret? More magic? That seemed like a rather suspicious sort of excuse. It should have been a clean case for Mum joking, lying or being insane, in ascending order of awfulness. If Mum had sent the letter herself, that would explain how it arrived at the letterbox without a stamp. A little insanity was far, far less improbable than the universe really working like that. Except that some part of Harry was utterly convinced that magic was real, and had been since the instant he saw the putative letter from the Hogwarts School of Witchcraft and Wizardry. Harry rubbed his forehead, grimacing. Don\\'t believe everything you think, one of his books had said. But this bizarre certainty... Harry was finding himself just expecting that, yes, a Hogwarts professor would show up and wave a wand and magic would come out. The strange certainty was making no effort to guard itself against falsification - wasn\\'t making excuses in advance for why there wouldn\\'t be a professor, or the professor would only be able to bend spoons. Where do you come from, strange little prediction? Harry directed the thought at his brain. Why do I believe what I believe? Usually Harry was pretty good at answering that question, but in this particular case, he had no clue what his brain was thinking. Harry mentally shrugged. A flat metal plate on a door affords pushing, and a handle on a door affords pulling, and the thing to do with a testable hypothesis is to go and test it. He took a piece of lined paper from his desk, and started writing. Dear Deputy Headmistress Harry paused, reflecting; then discarded the paper for another, tapping another millimetre of graphite from his mechanical pencil. This called for careful calligraphy. Dear Deputy Headmistress Minerva McGonagall, Or Whomsoever It May Concern: I recently received your letter of acceptance to Hogwarts, addressed to Mr. H. Potter. You may not be aware that my genetic parents, James Potter and Lily Potter (formerly Lily Evans) are dead. I was adopted by Lily\\'s sister, Petunia Evans-Verres, and her husband, Michael Verres-Evans. I am extremely interested in attending Hogwarts, conditional on such a place actually existing. Only my mother Petunia says she knows about magic, and she can\\'t use it herself. My father is highly sceptical. I myself am uncertain. I also don\\'t know where to obtain any of the books or equipment listed in your acceptance letter. Mother mentioned that you sent a Hogwarts representative to Lily Potter (then Lily Evans) in order to demonstrate to her family that magic was real, and, I presume, help Lily obtain her school materials. If you could do this for my own family it would be extremely helpful. Sincerely, Harry James Potter-Evans-Verres. Harry added their current address, then folded up the letter and put it in an envelope, which he addressed to Hogwarts. Further consideration led him to obtain a candle and drip wax onto the flap of the envelope, into which, using a penknife\\'s tip, he impressed the initials H.J.P.E.V. If he was going to descend into this madness, he was going to do it with style. Then he opened his door and went back downstairs. His father was sitting in the living-room and reading a book of higher maths to show how smart he was; and his mother was in the kitchen preparing one of his father\\'s favourite meals to show how loving she was. It didn\\'t look like they were talking to one another at all. As scary as arguments could be, not arguing was somehow much worse. \"Mum,\" Harry said into the unnerving silence, \"I\\'m going to test the hypothesis. According to your theory, how do I send an owl to Hogwarts?\" His mother turned from the kitchen sink to stare at him, looking shocked. \"I - I don\\'t know, I think you just have to own a magic owl.\" That should\\'ve sounded highly suspicious, oh, so there\\'s no way to test your theory then, but the peculiar certainty in Harry seemed willing to stick its neck out even further. \"Well, the letter got here somehow,\" Harry said, \"so I\\'ll just wave it around outside and call \\'letter for Hogwarts!\\' and see if an owl picks it up. Dad, do you want to come and watch?\" His father shook his head minutely and kept on reading. Of course, Harry thought to himself. Magic was a disgraceful thing that only stupid people believed in; if his father went so far as to test the hypothesis, or even watch it being tested, that would feel like associating himself with that... Only as Harry stumped out the back door, into the back garden, did it occur to him that if an owl did come down and snatch the letter, he was going to have some trouble telling Dad about it. But - well - that can\\'t really happen, can it? No matter what my brain seems to believe. If an owl really comes down and grabs this envelope, I\\'m going to have worries a lot more important than what Dad thinks. Harry took a deep breath, and raised the envelope into the air. He swallowed. Calling out Letter for Hogwarts! while holding an envelope high in the air in the middle of your own back garden was... actually pretty embarrassing, now that he thought about it. No. I\\'m better than Dad. I will use the scientific method even if it makes me feel stupid. \"Letter -\" Harry said, but it actually came out as more of a whispered croak. Harry steeled his will, and shouted into the empty sky, \"Letter for Hogwarts! Can I get an owl?\" \"Harry?\" asked a bemused woman\\'s voice, one of the neighbours. Harry pulled down his hand like it was on fire and hid the envelope behind his back like it was drug money. His whole face was hot with shame. An old woman\\'s face peered out from above the neighbouring fence, grizzled grey hair escaping from her hairnet. Mrs. Figg, the occasional babysitter. \"What are you doing, Harry?\" \"Nothing,\" Harry said in a strangled voice. \"Just - testing a really silly theory -\" \"Did you get your acceptance letter from Hogwarts?\" Harry froze in place. \"Yes,\" Harry\\'s lips said a little while later. \"I got a letter from Hogwarts. They say they want my owl by the 31st of July, but -\" \"But you don\\'t have an owl. Poor dear! I can\\'t imagine what someone must have been thinking, sending you just the standard letter.\" A wrinkled arm stretched out over the fence, and opened an expectant hand. Hardly even thinking at this point, Harry gave over his envelope. \"Just leave it to me, dear,\" said Mrs. Figg, \"and in a jiffy or two I\\'ll have someone over.\" And her face disappeared from over the fence. There was a long silence in the garden. Then a boy\\'s voice said, calmly and quietly, \"What.\" Chapter 10: Self Awareness, Part II All your base are still belong to Rowling.  And now you will sit through the Sorting Hat singing its version of Evanescence\\'s \"My Immortal\", which has never happened before. just kidding  ...he wondered if the Sorting Hat was genuinely conscious in the sense of being aware of its own awareness, and if so, whether it was satisfied with only getting to talk to eleven-year-olds once per year. Its song had implied so: Oh, I\\'m the Sorting Hat and I\\'m okay, I sleep all year and I work one day... When there was once more silence in the room, Harry sat on the stool and carefully placed onto his head the 800-year-old telepathic artefact of forgotten magic. Thinking, just as hard as he could: Don\\'t Sort me yet! I have questions I need to ask you! Have I ever been Obliviated? Did you Sort the Dark Lord when he was a child and can you tell me about his weaknesses? Can you tell me why I got the brother wand to the Dark Lord\\'s? Is the Dark Lord\\'s ghost bound to my scar and is that why I get so angry sometimes? Those are the most important questions, but if you\\'ve got another moment can you tell me anything about how to rediscover the lost magics that created you? Into the silence of Harry\\'s spirit where before there had never been any voice but one, there came a second and unfamiliar voice, sounding distinctly worried: \"Oh, dear. This has never happened before...\" What? \"I seem to have become self-aware.\" WHAT? There was a wordless telepathic sigh. \"Though I contain a substantial amount of memory and a small amount of independent processing power, my primary intelligence comes fr'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# powinniśmy otrzymać z powrotem tekst\n",
    "decode(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65ad88e5-7b71-4e5e-b93f-506dd8ea8354",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "długość tekstu: 16123\n",
      "długość tekstu w bajtach: 16123\n",
      "liczba tokenów po tokenizacji: 10601\n"
     ]
    }
   ],
   "source": [
    "print('długość tekstu:', len(text))\n",
    "print('długość tekstu w bajtach:', len(list(text.encode('utf8'))))\n",
    "print('liczba tokenów po tokenizacji:', len(tokens_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ee7246-8234-4104-b79a-51faf22a85b4",
   "metadata": {},
   "source": [
    "### Klasa tokenizer\n",
    "* korzystając z kodu powyżej zaimplementuj klasę Tokenizer\n",
    "* zaimplementuj funkcje:\n",
    "    * train\n",
    "    * encode\n",
    "    * decode\n",
    "    * find_most_frequent_pair\n",
    "    * replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8514678-214a-40af-82a0-03466c5e3105",
   "metadata": {},
   "source": [
    "#### Uwaga\n",
    "* Przed testmi implementacji warto zrestartować kernel, żeby być pewnym, że nie kożysta się ze starych zmiennych globalnych\n",
    "* Potem trzeba ponownie wywołać komórki z importem bibliotek i wczytaniem tekstu (dwie pierwsze komórki z kodem od góry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6b0f553a-5140-466b-8fbb-d8aa8fc39a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.vocab = {} # słownik tokenów - numer tokenu -> tekst tokenu\n",
    "        self.merges = {} # słownik połączeń - para tokenów -> nowy token\n",
    "        self.next_token_idx = 0 # indeks nowego tokenu (pierwszy wolny indeks)\n",
    "\n",
    "    def train(self, text, n_merges=100): # n_merges to liczba połączeń\n",
    "        self.vocab = {i: bytes([i]) for i in range(256)}\n",
    "        self.next_token_idx = 256\n",
    "        tokens_list = list(text.encode('utf8'))\n",
    "        self.merges = {}\n",
    "        # wykonaj zadaną liczbę połączeń, aktualizując listę tokenów, słownik merges i indeks nowego tokenu\n",
    "        #TODO\n",
    "\n",
    "        # aktualizacja słownika tokenów\n",
    "        for (t1, t2), tm in self.merges.items():\n",
    "            self.vocab[tm] = self.vocab[t1] + self.vocab[t2]\n",
    "\n",
    "    def encode(self, text):\n",
    "        # zamiana tekstu na listę tokenów\n",
    "        # TODO\n",
    "        return tokens_list\n",
    "\n",
    "    def decode(self, tokens_list):\n",
    "        # zamiana listy tokenów na tekst\n",
    "        # TODO\n",
    "        return #TODO\n",
    "\n",
    "    def find_most_frequent_pair(self, tokens_list):\n",
    "        # znajdź najczęstszą parę tokenów\n",
    "        # TODO\n",
    "        return #TODO\n",
    "\n",
    "    def replace(self, tokens_list, pair, new_token):\n",
    "        # zamiana pary tokenów na nowy token\n",
    "        # funkcja zwraca nową listę tokenów\n",
    "        new_tokens_list = []\n",
    "        # TODO\n",
    "        return new_tokens_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "40b4ac05-96ac-43ed-9d73-b7a3e00899a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tworzymy tokenizer\n",
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "295d6cdc-0390-4541-9e10-40415af84352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uczymy tokenizer\n",
    "tokenizer.train(text, n_merges=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d3960288-0df5-45fb-bb7d-4b65116d0c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(101, 32): 256, (32, 116): 257, (116, 32): 258, (100, 32): 259, (105, 110): 260, (115, 32): 261, (101, 114): 262, (257, 104): 263, (97, 110): 264, (46, 32): 265, (44, 32): 266, (111, 117): 267, (101, 110): 268, (97, 114): 269, (121, 32): 270, (260, 103): 271, (111, 32): 272, (116, 104): 273, (111, 110): 274, (111, 114): 275, (263, 256): 276, (262, 32): 277, (101, 259): 278, (101, 115): 279, (104, 105): 280, (104, 97): 281, (101, 108): 282, (102, 32): 283, (97, 32): 284, (105, 99): 285, (264, 259): 286, (119, 97): 287, (271, 32): 288, (34, 32): 289, (115, 116): 290, (257, 272): 291, (111, 119): 292, (101, 116): 293, (97, 108): 294, (73, 32): 295, (97, 258): 296, (121, 267): 297, (111, 109): 298, (105, 116): 299, (101, 100): 300, (269, 114): 301, (104, 256): 302, (108, 270): 303, (72, 301): 304, (111, 283): 305}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.merges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "78948c92-2073-4cde-818c-cf7bc1ef91aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'e ', 257: b' t', 258: b't ', 259: b'd ', 260: b'in', 261: b's ', 262: b'er', 263: b' th', 264: b'an', 265: b'. ', 266: b', ', 267: b'ou', 268: b'en', 269: b'ar', 270: b'y ', 271: b'ing', 272: b'o ', 273: b'th', 274: b'on', 275: b'or', 276: b' the ', 277: b'er ', 278: b'ed ', 279: b'es', 280: b'hi', 281: b'ha', 282: b'el', 283: b'f ', 284: b'a ', 285: b'ic', 286: b'and ', 287: b'wa', 288: b'ing ', 289: b'\" ', 290: b'st', 291: b' to ', 292: b'ow', 293: b'et', 294: b'al', 295: b'I ', 296: b'at ', 297: b'you', 298: b'om', 299: b'it', 300: b'ed', 301: b'arr', 302: b'he ', 303: b'ly ', 304: b'Harr', 305: b'of '}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "850c1823-f2c1-479c-8ed0-17ffc9f74882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 3, 4]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sprawdzamy czy zamiana działa\n",
    "tokenizer.replace([1,2,3,4], (1,2), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "be0a59bd-7aab-4d5d-a6cb-fd732062ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[66, 268, 101, 97, 273, 276, 109, 111, 274, 108, 105, 103, 104, 258, 103, 108, 260, 116, 261, 97, 257, 260, 270, 102, 114, 97, 103, 109, 268, 258, 305, 115, 105, 108, 118, 262, 266, 284, 102, 114, 97, 99, 116, 105, 274, 32, 305]\n"
     ]
    }
   ],
   "source": [
    "# tokenizujemy fragment tekstu\n",
    "tokens_list = tokenizer.encode(text[:70])\n",
    "print(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d4026edb-3e5e-47a3-bbcc-01fd5922e5c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beneath the moonlight glints a tiny fragment of silver, a fraction of '"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4807bf5-469c-4ccd-aaad-8312459c4b5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Beneath the moonlight glints a tiny fragment of silver, a fraction of '"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# i dekodujemy fragment tekstu (powinniśmy dostać z powrotem ten sam tekst)\n",
    "tokenizer.decode(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56b3481e-8ce8-4814-92ae-60627a5796d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16123\n",
      "10601\n"
     ]
    }
   ],
   "source": [
    "print(len(text))\n",
    "print(len(tokenizer.encode(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa7f9bc-d146-4410-8545-bd4687d5b24d",
   "metadata": {},
   "source": [
    "#### Uczenie na dłuższym korpusie tekstu po polsku i po angielsku\n",
    "* Wykorzystamy polski przekład Tajemnicy Baskervilleow\n",
    "    * Tekst pobrany z https://www.gutenberg.org/\n",
    "* Oraz dłuższy fragment (10 rozdziałów) HPMOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "42976961-718e-4ca4-ad6e-601aab1b079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/hpmor_chapters_1-10.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text_eng = response.text\n",
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/TajemnicaBaskervilleow.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text_pl = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "8178d212-7c5b-4e17-af60-8968ddf57758",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               TAJEMNICA BASKERVILLE'ÓW\n",
      "\n",
      "            Dziwne przygody Sherlocka Holmes\n",
      "\n",
      "\n",
      "                        PRZEZ\n",
      "\n",
      "                    Conana Doyle'a\n",
      "\n",
      "\n",
      "                Przekład z angielskiego\n",
      "\n",
      "                  EUGENII ŻMIJEWSKIEJ.\n",
      "\n",
      "\n",
      "                  Dodatek do „SŁOWA”\n",
      "\n",
      "                      WARSZAWA.\n",
      "\n",
      "                 DRUKIEM NOSKOWSKIEGO\n",
      "\n",
      "                 15. ulica Warecka 15.\n",
      "\n",
      "                        1902.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "I.\n",
      "\n",
      "Pan Sherlock Holmes.\n",
      "\n",
      "\n",
      "Pan Sherlock Holmes zwykł był wstawać późno, o ile nie czuwał przez\n",
      "całą noc, a zdarzało mu się to nieraz. Otóż owego dnia wstał\n",
      "wyjątkowo wcześnie. Jadł śniadanie. Stałem przy kominku. Schyliłem\n",
      "się i podniosłem laskę, którą nasz gość zostawił wczorajszego\n",
      "wieczoru. Był to kij gruby, z dużą gałką, utoczoną z drzewa; pod\n",
      "gałką była srebrna obrączka, a na niej napis: „Jakóbowi Mortimer\n",
      "M. R. C. S. od przyjaciół C. C. H.”, pod spodem zaś data:\n",
      "„r. 1884”. Taką laskę staroświecką, mocną, zapewniającą\n",
      "bezpiec\n"
     ]
    }
   ],
   "source": [
    "print(text_pl[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6f42cb61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Disclaimer J. K. Rowling owns Harry Potter, and no one owns the\n",
      "methods of rationality.\n",
      "This fic is widely considered to have really hit its stride\n",
      "starting at around Chapter 5. If you still don't like it after\n",
      "Chapter 10, give up.\n",
      "Please visit HPMOR DOT COM for\n",
      "* Easy email notification system, RSS feed, and Twitter feed for\n",
      "new chapters;\n",
      "* Current Author's Notes and progress updates;\n",
      "* Lovely fanmade bookstyle PDF version;\n",
      "* Adfree mirror of the text;\n",
      "* ePUB and MOBI etexts;\n",
      "* Ongoing podcast of the story;\n",
      "* Fan art in vast quantities;\n",
      "* Cameo list (characters named after fan artists);\n",
      "* Fanfanfiction of this fanfiction;\n",
      "* Fan music, songs, and animations;\n",
      "* Fan translations;\n",
      "* The OKCupid keyword for HPMOR readers;\n",
      "* Links to TV Tropes page and discussion forum;\n",
      "* Trigger warnings page (warnings about possible traumatic\n",
      "associations for some readers; to avoid spoilers for most readers,\n",
      "there are no warnings inside the main story);\n",
      "* How to learn everything the main character knows;\n"
     ]
    }
   ],
   "source": [
    "print(text_eng[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "55ee41e3-919e-4107-8a47-7002b87baeb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "długość polskiego tekstu: 233131\n",
      "długość angielskiego tekstu: 255326\n"
     ]
    }
   ],
   "source": [
    "print('długość polskiego tekstu:', len(text_pl))\n",
    "print('długość angielskiego tekstu:', len(text_eng))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78123f8e-56a1-4bd6-9454-d103b49012e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pl = Tokenizer()\n",
    "tokenizer_pl.train(text_pl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "042e1a15-cdbf-4c83-aec9-47aa429409e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "257 ł\n",
      "258 , \n",
      "259 o \n",
      "260 ę\n",
      "261 a \n",
      "262  p\n",
      "263 rz\n",
      "264  s\n",
      "265 ż\n",
      "266 ą\n",
      "267 nie\n",
      "268 \n",
      "\n",
      "\n",
      "269 --\n",
      "270 cz\n",
      "271 ś\n",
      "272  w\n",
      "273 ał\n",
      "274 . \n",
      "275 -- \n",
      "276 sz\n",
      "277 dz\n",
      "278  z\n",
      "279 .\n",
      "\n",
      "\n",
      "280 em\n",
      "281 st\n",
      "282 ó\n",
      "283 ni\n",
      "284 na\n",
      "285 ch\n",
      "286 ię\n",
      "287 ow\n",
      "288 e \n",
      "289 ć\n",
      "290 y \n",
      "291 ro\n",
      "292 ra\n",
      "293 i \n",
      "294 je\n",
      "295 rze\n",
      "296 le\n",
      "297  d\n",
      "298 .\n",
      "\n",
      "-- \n",
      "299  m\n",
      "300  t\n",
      "301 li\n",
      "302 go \n",
      "303 on\n",
      "304 by\n",
      "305 od\n",
      "306 ak\n",
      "307 an\n",
      "308 rzy\n",
      "309 am\n",
      "310 ci\n",
      "311 wi\n",
      "312 na \n",
      "313 dzi\n",
      "314  się\n",
      "315 ry\n",
      "316   \n",
      "317 ar\n",
      "318 , ż\n",
      "319 po\n",
      "320 ob\n",
      "321 ad\n",
      "322 er\n",
      "323 sk\n",
      "324 en\n",
      "325 or\n",
      "326 ej\n",
      "327 cie\n",
      "328 ka\n",
      "329 ta\n",
      "330 dzie\n",
      "331  po\n",
      "332 kt\n",
      "333 ko\n",
      "334 ów\n",
      "335  je\n",
      "336 był\n",
      "337 czy\n",
      "338 ego \n",
      "339  nie\n",
      "340 at\n",
      "341 wie\n",
      "342 mi\n",
      "343  na\n",
      "344 oś\n",
      "345  wy\n",
      "346 ją\n",
      "347 gł\n",
      "348 , że \n",
      "349  za\n",
      "350 wy\n",
      "351  na \n",
      "352 ny\n",
      "353 ir\n",
      "354 do\n",
      "355 go\n"
     ]
    }
   ],
   "source": [
    "for k, v in tokenizer_pl.vocab.items():\n",
    "    if k > 256:\n",
    "        print(k, v.decode('utf8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e804ea6-07c8-439a-b48a-dd234c63c8a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer_pl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer_pl\u001b[49m\u001b[38;5;241m.\u001b[39mencode(text_pl)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdługość polskiego tekstu:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(text_pl))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdługość polskiego tekstu w bajtach:\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mlist\u001b[39m(text_pl\u001b[38;5;241m.\u001b[39mencode(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf8\u001b[39m\u001b[38;5;124m'\u001b[39m))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer_pl' is not defined"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer_pl.encode(text_pl)\n",
    "print('długość polskiego tekstu:', len(text_pl))\n",
    "print('długość polskiego tekstu w bajtach:', len(list(text_pl.encode('utf8'))))\n",
    "print('liczba tokenów po tokenizacji:', len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb35cb7-09a1-4ff5-a0ac-5474509ed026",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens)==148913"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e5317c",
   "metadata": {},
   "source": [
    "#### Powtórz powyższe dla tekstu angielskiego\n",
    "* utwórz obiekt tokenizer_eng\n",
    "* naucz tokenizer_eng na tekście angielskim\n",
    "* wypisz słownik tokenów\n",
    "* tokenizuj tekst angielski\n",
    "* wypisz długość tekstu w bajtach i liczbę tokenów\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f726e49d-b991-4c4a-a192-a1d0f09d1b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Utworzyć obiekt tokenizer_eng i nauczyć go na tekście angielskim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7355b947-fc6f-4e34-9d7a-dddec02d75d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: b'\\x00', 1: b'\\x01', 2: b'\\x02', 3: b'\\x03', 4: b'\\x04', 5: b'\\x05', 6: b'\\x06', 7: b'\\x07', 8: b'\\x08', 9: b'\\t', 10: b'\\n', 11: b'\\x0b', 12: b'\\x0c', 13: b'\\r', 14: b'\\x0e', 15: b'\\x0f', 16: b'\\x10', 17: b'\\x11', 18: b'\\x12', 19: b'\\x13', 20: b'\\x14', 21: b'\\x15', 22: b'\\x16', 23: b'\\x17', 24: b'\\x18', 25: b'\\x19', 26: b'\\x1a', 27: b'\\x1b', 28: b'\\x1c', 29: b'\\x1d', 30: b'\\x1e', 31: b'\\x1f', 32: b' ', 33: b'!', 34: b'\"', 35: b'#', 36: b'$', 37: b'%', 38: b'&', 39: b\"'\", 40: b'(', 41: b')', 42: b'*', 43: b'+', 44: b',', 45: b'-', 46: b'.', 47: b'/', 48: b'0', 49: b'1', 50: b'2', 51: b'3', 52: b'4', 53: b'5', 54: b'6', 55: b'7', 56: b'8', 57: b'9', 58: b':', 59: b';', 60: b'<', 61: b'=', 62: b'>', 63: b'?', 64: b'@', 65: b'A', 66: b'B', 67: b'C', 68: b'D', 69: b'E', 70: b'F', 71: b'G', 72: b'H', 73: b'I', 74: b'J', 75: b'K', 76: b'L', 77: b'M', 78: b'N', 79: b'O', 80: b'P', 81: b'Q', 82: b'R', 83: b'S', 84: b'T', 85: b'U', 86: b'V', 87: b'W', 88: b'X', 89: b'Y', 90: b'Z', 91: b'[', 92: b'\\\\', 93: b']', 94: b'^', 95: b'_', 96: b'`', 97: b'a', 98: b'b', 99: b'c', 100: b'd', 101: b'e', 102: b'f', 103: b'g', 104: b'h', 105: b'i', 106: b'j', 107: b'k', 108: b'l', 109: b'm', 110: b'n', 111: b'o', 112: b'p', 113: b'q', 114: b'r', 115: b's', 116: b't', 117: b'u', 118: b'v', 119: b'w', 120: b'x', 121: b'y', 122: b'z', 123: b'{', 124: b'|', 125: b'}', 126: b'~', 127: b'\\x7f', 128: b'\\x80', 129: b'\\x81', 130: b'\\x82', 131: b'\\x83', 132: b'\\x84', 133: b'\\x85', 134: b'\\x86', 135: b'\\x87', 136: b'\\x88', 137: b'\\x89', 138: b'\\x8a', 139: b'\\x8b', 140: b'\\x8c', 141: b'\\x8d', 142: b'\\x8e', 143: b'\\x8f', 144: b'\\x90', 145: b'\\x91', 146: b'\\x92', 147: b'\\x93', 148: b'\\x94', 149: b'\\x95', 150: b'\\x96', 151: b'\\x97', 152: b'\\x98', 153: b'\\x99', 154: b'\\x9a', 155: b'\\x9b', 156: b'\\x9c', 157: b'\\x9d', 158: b'\\x9e', 159: b'\\x9f', 160: b'\\xa0', 161: b'\\xa1', 162: b'\\xa2', 163: b'\\xa3', 164: b'\\xa4', 165: b'\\xa5', 166: b'\\xa6', 167: b'\\xa7', 168: b'\\xa8', 169: b'\\xa9', 170: b'\\xaa', 171: b'\\xab', 172: b'\\xac', 173: b'\\xad', 174: b'\\xae', 175: b'\\xaf', 176: b'\\xb0', 177: b'\\xb1', 178: b'\\xb2', 179: b'\\xb3', 180: b'\\xb4', 181: b'\\xb5', 182: b'\\xb6', 183: b'\\xb7', 184: b'\\xb8', 185: b'\\xb9', 186: b'\\xba', 187: b'\\xbb', 188: b'\\xbc', 189: b'\\xbd', 190: b'\\xbe', 191: b'\\xbf', 192: b'\\xc0', 193: b'\\xc1', 194: b'\\xc2', 195: b'\\xc3', 196: b'\\xc4', 197: b'\\xc5', 198: b'\\xc6', 199: b'\\xc7', 200: b'\\xc8', 201: b'\\xc9', 202: b'\\xca', 203: b'\\xcb', 204: b'\\xcc', 205: b'\\xcd', 206: b'\\xce', 207: b'\\xcf', 208: b'\\xd0', 209: b'\\xd1', 210: b'\\xd2', 211: b'\\xd3', 212: b'\\xd4', 213: b'\\xd5', 214: b'\\xd6', 215: b'\\xd7', 216: b'\\xd8', 217: b'\\xd9', 218: b'\\xda', 219: b'\\xdb', 220: b'\\xdc', 221: b'\\xdd', 222: b'\\xde', 223: b'\\xdf', 224: b'\\xe0', 225: b'\\xe1', 226: b'\\xe2', 227: b'\\xe3', 228: b'\\xe4', 229: b'\\xe5', 230: b'\\xe6', 231: b'\\xe7', 232: b'\\xe8', 233: b'\\xe9', 234: b'\\xea', 235: b'\\xeb', 236: b'\\xec', 237: b'\\xed', 238: b'\\xee', 239: b'\\xef', 240: b'\\xf0', 241: b'\\xf1', 242: b'\\xf2', 243: b'\\xf3', 244: b'\\xf4', 245: b'\\xf5', 246: b'\\xf6', 247: b'\\xf7', 248: b'\\xf8', 249: b'\\xf9', 250: b'\\xfa', 251: b'\\xfb', 252: b'\\xfc', 253: b'\\xfd', 254: b'\\xfe', 255: b'\\xff', 256: b'e ', 257: b' t', 258: b't ', 259: b'in', 260: b'd ', 261: b'er', 262: b' th', 263: b's ', 264: b'ou', 265: b'an', 266: b'on', 267: b', ', 268: b'ar', 269: b'y ', 270: b'en', 271: b'th', 272: b'o ', 273: b'ing', 274: b'or', 275: b'. ', 276: b'al', 277: b' the ', 278: b'of', 279: b'ha', 280: b're', 281: b'you', 282: b'st', 283: b'ed ', 284: b'ing ', 285: b'at', 286: b'ed', 287: b'hi', 288: b'a ', 289: b'and ', 290: b'er ', 291: b' to ', 292: b'wa', 293: b'ow', 294: b'es', 295: b'all', 296: b'at ', 297: b'\"\\n', 298: b'it', 299: b'om', 300: b'.\\n', 301: b'arr', 302: b'el', 303: b' s', 304: b'Harr', 305: b'ac', 306: b'he ', 307: b'I ', 308: b'of ', 309: b'gh', 310: b'id', 311: b'ic', 312: b'le', 313: b'is', 314: b'ag', 315: b'ch', 316: b'oo', 317: b'e\\n', 318: b'or ', 319: b'you ', 320: b'il', 321: b'. \"', 322: b'ion', 323: b'Harry ', 324: b'us', 325: b'k ', 326: b'en ', 327: b've ', 328: b'was ', 329: b'as', 330: b'in ', 331: b'and', 332: b'li', 333: b' that ', 334: b\"'t \", 335: b'oul', 336: b'ot', 337: b'ev', 338: b'.\"\\n', 339: b'be', 340: b\"'s \", 341: b'\" ', 342: b'wi', 343: b'ent', 344: b'it ', 345: b'wh', 346: b'ad', 347: b'ol', 348: b'e th', 349: b'ab', 350: b'an ', 351: b'is ', 352: b'sa', 353: b'l ', 354: b'his ', 355: b'em'}\n"
     ]
    }
   ],
   "source": [
    "# TODO: Wyświetlić słownik tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "afd5a907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "długość angielskiego tekstu: 255326\n",
      "długość angielskiego tekstu w bajtach: 255326\n",
      "liczba tokenów po tokenizacji: 151373\n"
     ]
    }
   ],
   "source": [
    "# TODO: Tokenizować tekst angielski\n",
    "# TODO: Wyświetlić długość tekstu w bajtach i liczbę tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6eb55cd-0d92-4b2a-9f98-691afbddb8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(tokens)==151373"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b52b41f",
   "metadata": {},
   "source": [
    "#### Wykonaj tokenizację tekstu polskiego z wykorzystaniem tokenizatora nauczonego na tekście angielskim\n",
    "* wypisz długość tekstu w tokenach po tokenizacji angielskiego tokenizera\n",
    "* wypisz długość tekstu w tokenach po tokenizacji polskiego tokenizera\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fe214937-bce6-4049-9b8d-b5d3d0ae60c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Wykonać tokenizację tekstu polskiego z wykorzystaniem tokenizatora nauczonego na tekście angielskim\n",
    "# TODO: Wyświetlić długość tekstu w tokenach po tokenizacji angielskiego tokenizera\n",
    "# TODO: Wyświetlić długość tekstu w tokenach po tokenizacji polskiego tokenizera\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40764162-2a62-4128-a27d-a16131f3a18c",
   "metadata": {},
   "source": [
    "### Pytania\n",
    "1. Jakie tokeny uzyskaliśmy dla uczenia na tekście angielskim? A jakie dla uczenia na tekście polskim? Czemu są różne?\n",
    "2. Dla tekstu angielskiego liczba znaków i liczba bajtów jest taka sama. Dlaczego dla tekstu polskiego liczba bajtów jest większa?\n",
    "3. Na końcu wykorzystujemy tokenizator nauczony na tekście angielskim do tokenizacji tekstu polskiego. Jak ocenisz wynik? Czemu liczba tokenów różni się od liczby tokenów uzyskanej przy tokenizacji z wykorzystaniem tokenizatora nauczonego na tekście polskim?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b32b324d-181d-4dbc-9161-d1e3ea7f8a3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
