{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2128f48e-ab5f-4a05-a359-e499beb86a44",
   "metadata": {},
   "source": [
    "## Wykład 8 - Jak działa GPT?\n",
    "\n",
    "* dostęp przez API do modeli OpenAI\n",
    "* modele o otwartych wagach z wykorzystaniem HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d053b6-52c6-4a9b-aa57-49744425e0bf",
   "metadata": {},
   "source": [
    "### Dostęp przez API do modeli OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34aa065b-0e2f-4fb4-8d65-d0de5d9ac88c",
   "metadata": {},
   "source": [
    "#### Tworzenie klucza\n",
    "1. Tworzymy konto na https://platform.openai.com/ na mail PW i czekamy na dodanie do projektu związanego z przedmiotem\n",
    "2. Logujemy się na https://platform.openai.com/\n",
    "3. Wchodzimy w ustawienia (`Settings`) - kółko zębate w prawym górnym rogu\n",
    "4. Z menu po lewej wybieramy `API keys`\n",
    "5. Klikamy `Create new secret key` (zielony przycisk na górze po prawej)\n",
    "6. Wybieramy odpowiedni projekt i klikamy `Create secret key`\n",
    "7. Naciskamy `Copy` i kopiujemy klucz do schowka\n",
    "8. Tworzymy plik .env i umieszczamy w nim OPENAI_API_KEY=`<nasz klucz>`\n",
    "\n",
    "**Uwaga:** Nie należy nikomu udostępniać utworzonego klucza i umieszczać go np. w publicznych repozytoriach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15be56a0-fb4c-44ba-99cb-d5a03cedf661",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4925a34d-6d22-48a1-beda-5d16a5f1700d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ładujemy plik .env\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "562ced29-4eb9-406a-91f6-829029f020d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wczytujemy wartość klucza\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "#print(f\"API Key: {api_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d43f482e-ab76-4ef6-9ccd-f1ed552ec83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c0aad5-abd2-4128-8330-0a5695be72db",
   "metadata": {},
   "source": [
    "### Wykorzystanie chat completions\n",
    "* model - oznacza, z jakiego modelu korzystamy\n",
    "* będziemy korzystać z *gpt-4o-mini* ponieważ jest tani\n",
    "\n",
    "Do modelu zawsze wysyłamy listę `messages`:\n",
    "* jest to lista słowników\n",
    "* słowniki zawierają pola `role` (rola) i `content` (zawartość wiadomości)\n",
    "* dostępne role:\n",
    "    * system - ogólne informacje z opisem zadania, np. you are a helpfull assistant, jesteś chatbotem przyjmującym zamówienia w pizzerii ...\n",
    "    * user - użytkownik (człowiek), nasza wiadomość dla bota\n",
    "    * assistant - odpowiedź modelu\n",
    "* wiadomości `system` mogą być lub nie\n",
    "* zawsze ostatnią przesłaną wiadomością musi być wiadomość `user`\n",
    "* jeśli chcemy uzyskać kontynuację dłuższej konwersacji, to wiadomości `user` i `assistant` występują na przemian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89a3bc66-d121-4932-a8fa-3281476270b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"}\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4ddfd36-30ea-43f5-beb3-2f246e2eb47c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChatCompletion(id='chatcmpl-BZZJdMv89VWaoCWyoMItqNCI3XxoZ', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Silent thoughts entwined,  \\nLogic blooms in circuits bright,  \\nDreams of code arise.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747816933, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_54eb4bd693', usage=CompletionUsage(completion_tokens=19, prompt_tokens=13, total_tokens=32, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "print(completion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf9f2b60-acdc-45ef-834b-aa012025f5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Silent thoughts entwined,  \\nLogic blooms in circuits bright,  \\nDreams of code arise.', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ae3da6-58d1-4cff-bbe1-e4a3a333931b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silent thoughts entwined,  \n",
      "Logic blooms in circuits bright,  \n",
      "Dreams of code arise.\n"
     ]
    }
   ],
   "source": [
    "# wybieramy odpowiedź modelu\n",
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bab4fae-c924-4ecb-9fb3-6f3f73eca6f8",
   "metadata": {},
   "source": [
    "#### Dłuższa konwersacja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "967341db-f506-4337-a80a-fbfb9940d763",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"write a haiku about ai\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"Code whispers softly,  \\nThoughts of silicon and light,  \\nDreams in data flow. \"},\n",
    "        {\"role\": \"user\", \"content\": \"great! now translate into Polish\"},\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30061ad9-638a-451c-88fc-deaeb1b91492",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kod szepcze cicho,  \n",
      "Myśli z krzemu i światła,  \n",
      "Marzenia w danych.  \n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ee08b8-6565-40e2-bfe9-7190d91d98a5",
   "metadata": {},
   "source": [
    "#### Parametr max_tokens\n",
    "* określa ile maksymalnie tokenów model wygeneruje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "40c6c800-6a9c-4c15-af38-573959c5903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is your favourite animal? Answer with one word only.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_tokens=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c12bf82b-62fa-417b-b866-cba5bf7b305d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b9a0d7-d27b-4130-98de-4cbafd9e7ffa",
   "metadata": {},
   "source": [
    "#### Parametr temperatura\n",
    "* model zwraca rozkład prawdopodobieństwa następnego tokenu\n",
    "* możemy losować kolejny token z tego rozkładu (temperatura = 1) (tak robiliśmy na lab)\n",
    "* temperatura = 0 oznacza wybranie najbardziej prawdopodobnego tokenu\n",
    "* zwiększenie temperatury powyżej 1 powoduje większą losowość"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "45d3180c-d9e5-42b2-9876-73c8c3823817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n"
     ]
    }
   ],
   "source": [
    "# wyzyłamy zapytanie 10 razy z temperaturą 0\n",
    "for i in range(10):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is your favourite animal? Answer with one word only.\"}\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d7d0a48-93b4-412f-872f-f9f52297ed91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Otter.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Octopus.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Penguin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Wolf.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Octopus.\n",
      "Wolf.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Octopus.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Wolf.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dog.\n",
      "Dolphin.\n",
      "Elephant.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Penguin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Wolf.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Octopus.\n",
      "Elephant.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Octopus.\n",
      "Tiger.\n",
      "Dolphin.\n",
      "Wolf.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n",
      "Dolphin.\n"
     ]
    }
   ],
   "source": [
    "# i z temperaturą 1\n",
    "for i in range(100):\n",
    "    completion = client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": \"What is your favourite animal? Answer with one word only.\"}\n",
    "        ],\n",
    "        temperature=1,\n",
    "    )\n",
    "    print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e454d0d1-dad1-4df3-81b7-e13ddcb762f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Explain neural networks\"}\n",
    "    ],\n",
    "    temperature=2,\n",
    "    max_tokens=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cb73f814-f40c-4eac-9b91-3e82adc12598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks are computational models inspired by the human brain’s networks of neurons that are designed to recognize patterns and process information. They're a central technique in the field of artificial intelligence (AI), particularly innection.theamp lest ಹಾಗ ocasion genutzt akc быстрее фин kesiKwamamazaзнач депageno.dataset определитьnowôn厂 الألم-analysis.thê(((imize continuity sender cevaнатоск ouvert.keleté nég kennism 宣 qəbulлядrealikeun الزرا็น بسهولة友情链接사즐rak impeccable carbs.deslab tsakanin hükü задан deporte nek 判'lish\n"
     ]
    }
   ],
   "source": [
    "print(completion.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "409212d6-6b60-48a2-afdd-e82bf5bb671e",
   "metadata": {},
   "source": [
    "#### Logproby\n",
    "* możemy dostać bezpośrednio logproby (logarytmy prawdopodobieństw) dla najbardziej prawdopodobnych tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8667c70b-0fd2-4461-86a9-fe265d35632e",
   "metadata": {},
   "outputs": [],
   "source": [
    "completion_logprobs = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is your favourite animal? Answer with one word only.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    max_tokens=1,\n",
    "    logprobs=True, # ustawiamy na true\n",
    "    top_logprobs=5, # dla ilu najbardziej prawdopodobnych tokenów\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f0ecb654-a97f-4ae8-9233-03bfa897bd89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletion(id='chatcmpl-BZZLJGMkIZwrlyxf7dSggMzDW7u5q', choices=[Choice(finish_reason='length', index=0, logprobs=ChoiceLogprobs(content=[ChatCompletionTokenLogprob(token='D', bytes=[68], logprob=-0.12169364094734192, top_logprobs=[TopLogprob(token='D', bytes=[68], logprob=-0.12169364094734192), TopLogprob(token='Wolf', bytes=[87, 111, 108, 102], logprob=-3.1216936111450195), TopLogprob(token='Oct', bytes=[79, 99, 116], logprob=-3.9966936111450195), TopLogprob(token='Ele', bytes=[69, 108, 101], logprob=-4.1216936111450195), TopLogprob(token='Peng', bytes=[80, 101, 110, 103], logprob=-4.2466936111450195)])], refusal=None), message=ChatCompletionMessage(content='D', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1747817037, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_54eb4bd693', usage=CompletionUsage(completion_tokens=1, prompt_tokens=19, total_tokens=20, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completion_logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d031e4b6-2e8e-4691-afa6-519d06fb6358",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TopLogprob(token='D', bytes=[68], logprob=-0.12169364094734192),\n",
       " TopLogprob(token='Wolf', bytes=[87, 111, 108, 102], logprob=-3.1216936111450195),\n",
       " TopLogprob(token='Oct', bytes=[79, 99, 116], logprob=-3.9966936111450195),\n",
       " TopLogprob(token='Ele', bytes=[69, 108, 101], logprob=-4.1216936111450195),\n",
       " TopLogprob(token='Peng', bytes=[80, 101, 110, 103], logprob=-4.2466936111450195)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logprobs = completion_logprobs.choices[0].logprobs.content[0].top_logprobs\n",
    "logprobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c375037e-23d7-4df8-8895-00774b21bb7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D 0.89\n",
      "Wolf 0.04\n",
      "Oct 0.02\n",
      "Ele 0.02\n",
      "Peng 0.01\n"
     ]
    }
   ],
   "source": [
    "for top_logprob in logprobs:\n",
    "    print(top_logprob.token, \"{:.2f}\".format(np.exp(top_logprob.logprob)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8471628b-0658-4c10-8b9f-e49cd0f7a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# próbkujemy 100 razy\n",
    "completion = client.chat.completions.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"What is your favourite animal? Answer with one word only.\"}\n",
    "    ],\n",
    "    temperature=1,\n",
    "    n=100,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29edd0f1-5dc1-4559-b18e-919bf5f0ec49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'Dolphin.': 91, 'Wolf.': 2, 'Elephant.': 2, 'Penguin.': 1, 'Cheetah.': 1, 'Octopus.': 1, 'Quokka.': 1, 'Panda.': 1})\n"
     ]
    }
   ],
   "source": [
    "# i zliczamy odpowiedzi\n",
    "animals = []\n",
    "for choice in completion.choices:\n",
    "    animals.append(choice.message.content)\n",
    "\n",
    "counter = Counter(animals)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d725fb-ec1b-4234-a14c-a153abd5e586",
   "metadata": {},
   "source": [
    "### Inne możliwości\n",
    "* generacja obrazu\n",
    "* assystenci z dostępem do interpretera kodu i możliwością dołączania plików\n",
    "* generacja audio (czytanie tekstu)\n",
    "* embeddingi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1571fb87-af9b-47c5-a8c8-b245d074b085",
   "metadata": {},
   "source": [
    "### Tworzenie chatbota"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "564b3114-fe70-4355-a0c8-052fa936c1cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Wpisz coś:  hej\n"
     ]
    }
   ],
   "source": [
    "# wczytywanie danych od użytkownika\n",
    "text = input(\"Wpisz coś: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1bd1718f-5e04-4b30-b44f-977eca206d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hej'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ce2428ff-f4cf-45e5-b1b6-7af0c87bce00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# funkcja input zwraca typ string\n",
    "type(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1987fd0c-a652-4465-8e8a-4b7969939929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hej\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Hej! Hur kan jag hjälpa dig idag?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  hej po polsku\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Cześć! Jak mogę Ci dzisiaj pomóc?\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  w jakim języku odpowiedziałeś najpierw?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: Odpowiedziałem najpierw po szwedzku.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "User:  koniec\n"
     ]
    }
   ],
   "source": [
    "# zapętlamy interakcje aż do wpisania \"koniec\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}\n",
    "]\n",
    "\n",
    "while True:\n",
    "    user_input = input(\"User: \")\n",
    "    \n",
    "    if user_input.lower() == \"koniec\":\n",
    "        break\n",
    "\n",
    "    # Dodajemy wiadomość użytkownika do historii wiadomości\n",
    "    messages.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    # wysyłamy zapytanie przez OpenAI API\n",
    "    response =  client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    # Wybieramy odpowiedź assystenta\n",
    "    reply = response.choices[0].message.content\n",
    "    \n",
    "    # Drukujemy odpowiedź\n",
    "    print(\"Assistant:\", reply)\n",
    "    # i dodajemy ją do listy wiadomości\n",
    "    messages.append({\"role\": \"assistant\", \"content\": reply})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5ccff1-54a7-4915-90ee-0f3bd8177453",
   "metadata": {},
   "source": [
    "### HuggingFace\n",
    "* dostęp do wielu modeli o otwartych wagach\n",
    "* lokalnie możemy uruchomić tylko małe modele\n",
    "* ograniczeniem jest ilość RAMu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "53688a73-4040-48c7-9e37-d5400319789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ania/dydaktyka_LLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "072bc283-8967-4b8b-9bf2-e82a1c7e74de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# malutki model destylowany z gpt-2\n",
    "generator = pipeline(\"text-generation\", model=\"sshleifer/tiny-gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "156dbb47-07da-4300-bfc6-f5988c5e4448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = generator(\"The future of AI is\", num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "98316af6-0f96-4d87-9450-51124e012cf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The future of AI isatisf trilogy Money credibilitypress AmphRocketpress circumcised Brew Money ProbScene Amph Hancock intermittentohooother dispatchSher circumcised Observpress Rh hauledatisf Daniel confirScenereement Habit dispatch ONE heiroho trilogy Participation subst ONEiken trilogyRocketohopress Prob\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb600211-c976-4849-9d46-ce774f69944a",
   "metadata": {},
   "source": [
    "* mały model, słaba jakość"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac8a642-e73c-4913-b82a-08a5a96cd643",
   "metadata": {},
   "source": [
    "#### Tokenizacja Bielik\n",
    "\n",
    "* https://huggingface.co/speakleash/Bielik-7B-v0.1\n",
    "* wagi modelu (możemy podejrzeć rozmiar w `files and versions`) mają w sumie około 15 GB\n",
    "* heurystyka: potrzeba około 4x tyle ramu co wagi do inferencji\n",
    "* a do uczenia jeszcze więcej\n",
    "* ale możemy podejrzeć sobie tokenizację\n",
    "* Bielik był douczany z Mistrala (model francuski)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a50cfa2-e011-46b9-a056-b04f5e2cc526",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bfad793e-fa14-428f-9e39-a54b8db05f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"speakleash/Bielik-7B-v0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "761396e4-5e7b-4b10-b726-dae6d772396f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['▁w', '▁Sz', 'cz', 'eb', 'r', 'zes', 'z', 'yn', 'ie', '▁chr', 'zą', 'sz', 'cz', '▁br', 'z', 'mi', '▁w', '▁tr', 'z', 'cin', 'ie']\n"
     ]
    }
   ],
   "source": [
    "text = \"w Szczebrzeszynie chrząszcz brzmi w trzcinie\"\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "print(\"Tokens:\", tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "18cdc6d3-5f3d-4fcf-8950-b33e56f6c7b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w\n",
      " Szcz\n",
      "ebr\n",
      "zes\n",
      "zyn\n",
      "ie\n",
      " chr\n",
      "zą\n",
      "sz\n",
      "cz\n",
      " brz\n",
      "mi\n",
      " w\n",
      " trz\n",
      "cin\n",
      "ie\n"
     ]
    }
   ],
   "source": [
    "# porównanie z gpt-4o-mini\n",
    "import tiktoken \n",
    "encoding = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "for token in encoding.encode(text):\n",
    "    print(encoding.decode([token]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a316abb-e6a8-4cc5-9d0f-5faed40801cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dydaktyka_LLM",
   "language": "python",
   "name": "dydaktyka_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
