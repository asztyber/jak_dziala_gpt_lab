{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6e3f0fe-d0b3-4d1a-8dba-05e64034d2ed",
   "metadata": {},
   "source": [
    "## LAB 4 Jak działa GPT\n",
    "#### Predykcja następnego znaku w tekście z wykorzystaniem torch.nn.Embedding\n",
    "\n",
    "Celem laboratorium jest:\n",
    "\n",
    "* zapoznanie z modułem torch.nn\n",
    "* zapoznanie z warstwą osadzeń wektorowych (embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c967c847-90fc-4ce7-8b2d-180a1bf7c265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c2aa33f-17b7-43d2-8727-f14e15ffe1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_printoptions(precision=4, sci_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5d254c7-f49a-42c7-9900-db38119c695b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/hpmor_part.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "253b1d8e-7f14-4eb7-940d-026a367abbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(text)))\n",
    "n_tokens = len(chars) # liczba znaków\n",
    "idx_to_ch = {i: c for i, c in enumerate(chars)}\n",
    "ch_to_idx = {c: i for i, c in enumerate(chars)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082bbac-b793-4c2e-860b-baab314d143e",
   "metadata": {},
   "source": [
    "##### Generacja tekstu\n",
    "* Teraz w funkcji generującej tekst wykorzystujemy model do wyznaczenia logitów.\n",
    "* Prawdopodobieństwa wyznaczamy z wykorzystamiem F.softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af6299a-0fa4-44c2-8f2d-5aa3171ee67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_seq, model, max_size):\n",
    "    '''\n",
    "    Funkcja generuje tekst.\n",
    "    start_seq (str) - początek tekstu, podany przez użytkownika\n",
    "    norm_counts - znormalizowana macierz zliczeń\n",
    "    max_size (int) - zadana długość tekstu\n",
    "    '''\n",
    "    for i in range(max_size):\n",
    "        last_ch = start_seq[-1]\n",
    "        logits = model(torch.tensor(ch_to_idx[last_ch], device=device)) #<- zmiana\n",
    "        probs = F.softmax(logits, dim=0) #<- zmiana\n",
    "        probs = probs.cpu().detach().numpy()\n",
    "        next_ch = idx_to_ch[np.random.choice(n_tokens, p=probs)]\n",
    "        start_seq += next_ch\n",
    "    return start_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4f4e5-0139-4667-b58f-e59b289b49a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_to_tokens(idx):\n",
    "    '''\n",
    "    funkcja zamienia listę pozycji w tekście (o długości równej rozmiar batcha) na tensor o wymiarach \n",
    "    (rozmiar batcha x 1) zawierający indeksy tokenów\n",
    "    '''\n",
    "    x = [text[i] for i in idx]\n",
    "    x = [ch_to_idx[xx] for xx in x]\n",
    "    x = torch.tensor(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4543f7-ff1b-4222-97e6-e156dc55d5ef",
   "metadata": {},
   "source": [
    "##### Dane uczące\n",
    "* Zamiast kodowania one-hot będziemy stosować **indeksy** tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77d4f33-89d5-4b3b-b039-e705ec3d9b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(batch_size=8):\n",
    "    '''\n",
    "    funkcja zwraca batch danych uczących\n",
    "    x i y to tensory o wymiarach (rozmiar batcha x rozmiar słownika) zawierające indeksy tokenów\n",
    "    '''\n",
    "    rand_idx = np.random.randint(0, len(text) - 1, size=batch_size)\n",
    "    x = idx_to_tokens(rand_idx)\n",
    "    y = idx_to_tokens(rand_idx + 1)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2ef59-7603-45e7-a9c8-08ce7056cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40fd100-efa5-4ede-9376-d4602af5bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2c9725-f806-4b73-a7ea-c8867cdef31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d67a74c-b0ad-484a-b65d-66f5c5d87b2e",
   "metadata": {},
   "source": [
    "#### Embedding Model\n",
    "* Zaimplementować klasę EmbeddingModel\n",
    "* Model ma jedną warstwę typu Embedding o wymiarze liczba tokenów x liczba tokenów"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddde51d-58d8-4318-aebd-0e3270adcdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module): \n",
    "    # TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb9c43a-fd67-4f1a-ab80-c6a94b993085",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c556a869-647a-458b-b4f9-447b30e2923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ac04ec-7814-4e14-82ad-fc8dc610115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf082ec-63a0-4990-b1b0-f0937645c67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przenosimy model na gpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = EmbeddingModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf575d6-2a5d-40b7-877d-554c1fcfa141",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD(model.parameters(), lr=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d23c3-35f2-4587-ac1c-145574c5425f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1500\n",
    "batch_size = 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90baeb33-76b7-44b0-883e-a599c52a5e02",
   "metadata": {},
   "source": [
    "#### Zaimplementować pętlę uczenia\n",
    "* Należy wzorować się na przykładzie z wykładu\n",
    "* W przeciwieństwie do wykładu mamy dane w batchach\n",
    "* Każdy batch po wygenerowaniu trzeba przenieść na device\n",
    "* Powinno dać się uzyskać stratę na poziomie około 2.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eab684b-c4b3-4823-a672-1b173fd97ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab05f22a-e6b2-4975-adce-7b32641e641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generate_text(\"T\", model, 200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17dc72-564c-4c9b-8ef8-4936f84a981b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dydaktyka_LLM",
   "language": "python",
   "name": "dydaktyka_llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
