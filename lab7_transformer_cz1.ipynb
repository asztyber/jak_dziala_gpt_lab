{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e50a4c4a-4063-4e81-9275-fe326591175b",
   "metadata": {},
   "source": [
    "### Laboratorium 7 Jak działa GPT?\n",
    "#### Implemetacja transformera część 1\n",
    "\n",
    "Imię i nazwisko: ..........................\n",
    "\n",
    "Punktacja:\n",
    "* 6 pkt. prawidłowa implementacja \n",
    "* 2 pkt. wnioski\n",
    "\n",
    "Zaimplementujemy:\n",
    "* model oparty o Embedding (to już było), ale przetwarzający sekwencje znaków o długości *seq_len*\n",
    "    * ten model dalej korzysta tylko z ostatniego znaku\n",
    "    * celem jest oswojenie się z kształtem danych na znanym modelu\n",
    "* model posiadający jedną głowę atencji\n",
    "* model o wielu głowach atencji\n",
    "\n",
    "Model będziemy dalej rozwijać na kolejnym laboratorium. Kod z dzisiaj będzie używany jako punkt startowy i potrzebny za tydzień."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9418073-0cfe-4f4b-9536-643d45e69784",
   "metadata": {},
   "source": [
    "#### Źródła\n",
    "* https://youtu.be/kCc8FmEb1nY?si=wYbFi5JB3x-R8375\n",
    "* https://github.com/karpathy/nanoGPT\n",
    "* https://arena-chapter1-transformer-interp.streamlit.app/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da88d5f-9e38-4be9-8959-0b44ec6a179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a7068-c900-43a1-8a36-31fb08fd2a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8a6c01-0f50-4d52-a9a9-c0cb5d0f042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to jest do assertów, proszę zignorować\n",
    "def set_seeds(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7f33ff-5d83-4a24-bf24-48f382b5ecd6",
   "metadata": {},
   "source": [
    "#### Przygotowanie danych uczących"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b906b538-46d3-45a7-88f6-7e96d3cff217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# jako tekst ponownie wykorzystamy HPMOR rozdziały 1-10\n",
    "# Eliezer Yudkowsky, Harry Potter and the Methods of Rationality https://hpmor.com/\n",
    "url = \"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/data/hpmor_chapters_1-10.txt?raw=true\"\n",
    "response = requests.get(url)\n",
    "text = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82951980-6ae4-4486-8549-e84d929ca6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78419332-d6a0-48ac-967d-3a607e2ed2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "characters = sorted(set(text))\n",
    "vocab_size = len(characters)\n",
    "idx_to_ch = {i: c for i, c in enumerate(characters)}\n",
    "ch_to_idx = {c: i for i, c in enumerate(characters)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab96c0-314f-4687-9341-f1d1163190da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unikalne znaki w tekście\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6833c1e-80cc-48b9-b130-dd3b68ffaba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce2385f-bca7-4f5e-99fb-ee57cc84bc10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def code(text):\n",
    "    # zamienia tekst na listę tokenów (indeksów)\n",
    "    return [ch_to_idx[c] for c in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8a19cb-9ca8-47df-9b57-d54d3bacb83a",
   "metadata": {},
   "outputs": [],
   "source": [
    "code(\"ala ma kota\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e928bfac-0cc9-4d33-b4e9-78b0bc77edfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(tokens):\n",
    "    # zamienia listę tokenów (indeksów) na tekst\n",
    "    return ''.join(idx_to_ch[i] for i in tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9e454c-06ee-42c6-9c84-df2cfa353435",
   "metadata": {},
   "outputs": [],
   "source": [
    "decode([53, 64, 53, 1, 65, 53, 1, 63, 67, 72, 53])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3dacb3f-14d8-484d-a6de-3422ed029c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# kodujemy cały tekst (zamieniamy tekst na listę tokenów)\n",
    "text_coded = code(text)\n",
    "# tensor zawierający dane uczące\n",
    "train = torch.tensor(text_coded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a4423b-8847-4791-9794-ea37619a838e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data, seq_len=8, batch_size=4):\n",
    "    '''\n",
    "    Funkcja zwraca batch danych.\n",
    "    data (tensor) - dane uczące\n",
    "    seq_len (int) - długość sekwencji\n",
    "    batch_size (int) - rozmiar batcha\n",
    "\n",
    "    X - tensor o kształcie (batch_size, seq_len)\n",
    "    y - tensor o kształcie (batch_size, seq_len)\n",
    "    '''\n",
    "    n = len(data)\n",
    "    starts = np.random.randint(0, n - seq_len, batch_size)\n",
    "    X = torch.stack([data[s:s + seq_len] for s in starts])\n",
    "    y = torch.stack([data[s + 1: s + seq_len + 1] for s in starts])\n",
    "    return X.to(device), y.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2369486-8105-42ac-908a-e832276c06dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = get_batch(train, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403bad50-e6b6-4d90-894d-82f43842bab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tak wyglądają dane uczące\n",
    "# y zawiera następny token dla każdego tokenu w X (nie tylko dla ostatniego tokenu sekwencji!)\n",
    "# widzimy, że sekwencje w y są przesunięte o 1 względem sekwencji w X\n",
    "x, y = get_batch(train)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38efc883-e575-4a9d-bc29-da086a861c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zamieniamy tokeny na znaki i wypisujemy sekwencje x: ostatni token sekwencji y\n",
    "for xi, yi in zip(x, y):\n",
    "    print(decode(xi.tolist()), \": \", decode([yi.tolist()[-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ac8c47-23d7-432d-84f0-26b26f3dc419",
   "metadata": {},
   "source": [
    "#### Generacja tekstu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0269815-f673-445c-a7fc-032f389072d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(start_seq, model, max_size, seq_len):\n",
    "    '''\n",
    "    Funkcja generuje tekst.\n",
    "    start_seq (str) - początek tekstu, podany przez użytkownika\n",
    "    model - sieć neuronowa\n",
    "    max_size (int) - zadana długość tekstu\n",
    "    seq_len (int) - długość sekwencji podawanej na wejście modelu\n",
    "    '''\n",
    "    for i in range(max_size):\n",
    "        x = code(start_seq[-seq_len:]) #<- zmiana - ograniczamy dane wejściowe do ostatnich seq_len tokenów\n",
    "        logits = model(torch.tensor([x], device=device)) #<- zmiana, x umieszczamy w [], bo jest to tensor o kształcie (1, seq_len)\n",
    "        probs = F.softmax(logits, dim=-1) #<- zmiana dim=-1 - softmax jest obliczany po ostatnim wymiarze\n",
    "        probs = probs[0, -1, :].cpu().detach().numpy() # <- zmiana, probs ma teraz kształt (batch_size, seq_len, vocab_size)\n",
    "        # probs[0, -1, :] - wybieramy element batcha o indeksie 0 (jedyny)i sekwencji o indeksie -1 (ostatni token) \n",
    "        next_ch = idx_to_ch[np.random.choice(vocab_size, p=probs)]\n",
    "        start_seq += next_ch\n",
    "    return start_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c34837-0868-4481-9e9b-a2b5ef0b0b06",
   "metadata": {},
   "source": [
    "### Model oparty tylko o warstwę embedding\n",
    "* uwaga: to jest to samo co robiliśmy na lab 2 i lab 4, czyli przewidujemy znak tylko na podstawie poprzedniego znaku\n",
    "* ale kształt danych wejściowych jest już dostosowany do wykorzystania większej liczby elementów sekwencji\n",
    "* z innych elementów sekwencji korzystamy w kolejnych krokach\n",
    "\n",
    "\n",
    "### Zaimplementować EmbeddingModel\n",
    "* model ma jedną warstwę embedding\n",
    "* warstwa embedding ma kształt (*vocab_size*, *vocab_size*)\n",
    "* warstwa embedding jest warstwą typu nn.Embedding\n",
    "* warstwa embedding zwraca logity o kształcie (*batch_size*, *seq_len*, *vocab_size*)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c29f2f7-82d9-4049-b951-e9d6ebd679cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__() \n",
    "         #TODO\n",
    "\n",
    "    def forward(self, X):\n",
    "         #TODO\n",
    "        return logits   #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e341d3-6f5a-49c0-bb74-1b14e6c9b239",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EmbeddingModel(vocab_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e46536d-4042-4b7d-871e-41a581b66493",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wypisujemy kształty parametrów modelu\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a062070e-79fb-4c88-9a5a-98d91afdc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Liczba parametrów modelu: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac74f928-7119-43d4-bb1b-c6e6118d9a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyjście modelu ma kształt (*batch_size*, *seq_len*, *vocab_size*)\n",
    "y_hat = model(x)\n",
    "y_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488192f-4cec-45fc-b54d-763ffe0585c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "seq_len = 8\n",
    "n_steps = 1500\n",
    "model = EmbeddingModel(vocab_size).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff947e42",
   "metadata": {},
   "source": [
    "### Uzupełnić pętlę uczenia\n",
    "* argumenty F.cross_entropy (logity i y) powinny mieć kształt (*batch_size* * *seq_len*, *vocab_size*)\n",
    "* wskazówka: użyj .view do zmiany kształtu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1b16ac-2ed9-4e16-975d-d787d7569ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size):\n",
    "    losses = []\n",
    "    for step in range(n_steps):\n",
    "        optimizer.zero_grad()\n",
    "        x, y = get_batch(train, batch_size=batch_size)\n",
    "        logits = model(x)\n",
    "        loss = #TODO\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "    \n",
    "        if step % 100 == 0:\n",
    "            print(f\"Krok {step}: Loss = {loss.item():.4f}\")\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c516c",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "651ab549-4f9e-44fa-aef5-9538366340bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f128cf06-5403-42db-8eaa-66ed7e0a3907",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a72fba-37e7-4362-948f-366831f62cf0",
   "metadata": {},
   "source": [
    "### Atencja o jednej głowie (Attention head)\n",
    "\n",
    "Atencja o jednej głowie jest zdefiniowana następująco:\n",
    "\n",
    "$$Q = X W_Q,\\quad K = X W_K,\\quad V = X W_V$$\n",
    "$$\\text{scores} = \\frac{QK^{T}}{\\sqrt{d_k}}$$\n",
    "$$ \\text{mask}_{ij} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } j \\leq i \\\\\n",
    "0 & \\text{if } j > i\n",
    "\\end{cases} \\qquad \\text{macierz trójkątna dolna}$$\n",
    "$$\\text{masked\\_scores}_{ij} = \\begin{cases}\n",
    "\\text{scores}_{ij} & \\text{if } \\text{mask}_{ij} = 1 \\\\\n",
    "-\\infty & \\text{if } \\text{mask}_{ij} = 0\n",
    "\\end{cases} $$\n",
    "$$\\text{attention} = \\text{softmax}\\left(\\text{masked\\_scores}\\right)$$\n",
    "$$ \\text{weighted\\_values} = \\text{attention} \\cdot V $$\n",
    "$$ out = \\text{weighted\\_values} \\cdot W_{out} $$\n",
    "\n",
    "Implementujemy klasę **AttentionHead** (rysunek po lewej) oraz **AttentionHeadModel** (rysunek po prawej) według poniższego schematu:\n",
    "<img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/attention_head.png?raw=true\" alt=\"Atencja o jednej głowie\" width=\"800\" height=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6462acfc",
   "metadata": {},
   "source": [
    "### Zaimplementować klasę AttentionHead\n",
    "* pomocny jest przykład z wykładu 7\n",
    "* mnożenie przez macierze *Wk*, *Wq*, *Wv*, *Wout* jest realizowane przez warstwy nn.Linear\n",
    "    * macierze *Wk*, *Wq*, *Wv* mają kształt (*d_model*, *d_head*)\n",
    "    * macierz *Wout* ma kształt (*d_head*, *d_model*)\n",
    "    * wykorzystujemy tylko macierze wag bez biasu (argument bias=False w konstruktorze nn.Linear)\n",
    "* do wyznaczenia $K^T$ można użyć .transpose(-2, -1)\n",
    "* wykorzystać funkcję torch.tril do tworzenia maski\n",
    "* wykorzystać funkcję .masked_fill do maskowania wartości w macierzy *scores*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0b87be-9690-469b-8605-603c12e0cf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, d_model, d_head):\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.d_head = d_head\n",
    "        self.Wk = #TODO\n",
    "        self.Wq = #TODO\n",
    "        self.Wv = #TODO\n",
    "        self.Wout = #TODO\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, _ = x.shape\n",
    "        K = #TODO    \n",
    "        Q = #TODO\n",
    "        V = #TODO\n",
    "        scores = #TODO\n",
    "        mask = #TODO\n",
    "        masked_scores = #TODO\n",
    "        attention = #TODO\n",
    "        weighted_values = #TODO\n",
    "        out = #TODO\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608fc5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 16 # teraz mamy jedną głowę, więc d_model = d_head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62bd3ff-97ee-4403-b606-ee85b9509f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(42)\n",
    "x = torch.randn(batch_size, seq_len, d_model, device=device)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b044fdf4-4c26-42c0-97d7-357ce6f2b10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AttentionHead(d_model, d_head).to(device)\n",
    "out = model(x)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a4cf71-76ed-4f89-97f1-8f8e5619c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([2, 8, 16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35376391-78bb-43bf-9953-ec6409d484db",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([0.2119,  0.1579,  0.0388, -0.0663,  0.0123,  0.0133,  0.0630,  0.0521], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14003a89",
   "metadata": {},
   "source": [
    "### Zaimplementuj klasę AttentionHeadModel\n",
    "(rysunek powyżej)\n",
    "Model ten jest zbudowany z:\n",
    "* warstwy embedding o kształcie (*vocab_size*, *d_model*)\n",
    "* warstwy AttentionHead \n",
    "* warstwy liniowej out o kształcie (*d_model*, *vocab_size*) (typu nn.Linear, ta warstwa ma bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae800cbd-9287-4a22-a0af-975c0f6e4b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHeadModel(nn.Module):\n",
    "    def __init__(self, vocab_size, d_head, d_model):\n",
    "        super(AttentionHeadModel, self).__init__()\n",
    "        self.emb = #TODO\n",
    "        self.attention_head = #TODO\n",
    "        self.linear_out = #TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b77bf94-f7f0-407b-947f-7a07c30215ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(37)\n",
    "batch_size = 256\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 16\n",
    "n_steps = 1500\n",
    "model = AttentionHeadModel(vocab_size, d_head, d_model).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5439f0-82d7-4fec-a704-b5f6dc1120f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea43b035-32b9-46f4-be93-6e1d7beb6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([256, 8, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d82661-0d61-432d-a012-dd8a882685c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([-0.0923, -0.0804,  0.0360,  0.0703,  0.0691,  0.0821,  0.0501,  0.0606], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1917db-1cf7-4bb9-92bf-4ab7f661a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22b35d2-72d0-4434-991c-86c9ebb1f874",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Liczba parametrów modelu: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8c7620-daed-4c37-a25f-d79c42ea5a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4043c85d-15ad-4581-8090-c58646628558",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec3a16c-7b6a-4c81-a5e4-c3fc6883fdec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f15abb-c66d-448e-98e1-64d0423f6959",
   "metadata": {},
   "source": [
    "### Atencja o wielu głowach (Multihead attention)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb9a7d51-a722-42ec-8b01-231ae83b45a5",
   "metadata": {},
   "source": [
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "    <img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/multi_head_attention_simple.png?raw=true\" alt=\"Struktura sieci\" width=\"500\">\n",
    "    <img src=\"https://github.com/asztyber/jak_dziala_gpt_lab/blob/main/pic/multihead.png?raw=true\" alt=\"Atencja o wielu głowach\" width=\"500\">\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f5f28c",
   "metadata": {},
   "source": [
    "### Zaimplementuj klasę MultiHeadAttention\n",
    "\n",
    "(żółta część na rysunku powyżej)\n",
    "* model ma wiele (*n_heads*) głów atencji\n",
    "* każda głowa atencji jest typu AttentionHead\n",
    "* wykorzystać nn.ModuleList do przechowywania głów atencji\n",
    "* wyjścia wszystkich głów atencji są sumowane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28959870-628e-42e5-ae53-387c0ad0e546",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, n_heads, d_head, d_model):\n",
    "        super().__init__()\n",
    "        self.attention_heads = #TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3018f5d",
   "metadata": {},
   "source": [
    "### Zaimplementuj klasę MultiHeadModel\n",
    "* w odróżnieniu od AttentionHeadModel, MultiHeadModel ma wiele głów atencji\n",
    "* model ten jest zbudowany z:\n",
    "    * warstwy embedding o kształcie (*vocab_size*, *d_model*)\n",
    "    * warstwy MultiHeadAttention <- **jedyna zmiana w porównaniu do AttentionHeadModel**\n",
    "    * warstwy liniowej out o kształcie (*d_model*, *vocab_size*) (typu nn.Linear, ta warstwa ma bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07599f62-88f9-4a39-a3a1-9ff77f7f5703",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadModel(nn.Module):\n",
    "    def __init__(self, n_heads, vocab_size, d_head, d_model):\n",
    "        super().__init__()\n",
    "        self.emb = #TODO\n",
    "        self.linear_out = # TODO\n",
    "        self.multi_head_attention = #TODO\n",
    "\n",
    "    def forward(self, x):\n",
    "        #TODO\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7e667-43fd-4e2b-ae7c-6e26235fab93",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seeds(28)\n",
    "batch_size = 256\n",
    "seq_len = 8\n",
    "d_model = 16\n",
    "d_head = 4\n",
    "n_heads = 4\n",
    "model = MultiHeadModel(n_heads, vocab_size, d_head, d_model).to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a825f88-82a9-4f89-8a33-8d1754540f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randint(0, vocab_size, (batch_size, seq_len), device=device)\n",
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ff0a2d-e70e-44cd-af5c-a91f841ad260",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert out.shape == torch.Size([256, 8, 79])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1f13f5-7bd2-46af-9710-49a0b3d6a855",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert torch.allclose(out[0, :, 0], torch.tensor([0.2087, 0.0654, 0.0244, 0.0091, 0.0283, 0.0567, 0.0157, 0.0409], device=device), atol=10**(-4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0e59bb1-bcdc-4b0d-a979-5a56a2f6feab",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa9ba8-bc2d-4723-a872-7ae07906ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'Liczba parametrów modelu: {total_params}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "076b24fc-1ae4-4e30-b78a-ab142960d727",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = train_loop(model, optimizer, n_steps, batch_size, seq_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "474de3b9-db8d-4411-82ab-0f02116bfb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_text('T', model, 100, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897978d6-2bc7-4fa0-b29e-d44e1088de9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9732bd6d-3fa7-463c-80d1-0224f1c03b50",
   "metadata": {},
   "source": [
    "### Wnioski\n",
    "1. Porównaj tekst wygenerowany za pomocą EmbeddingModel, AttentionHeadModel i MultiHeadModel oraz wartości funkcji straty (loss) i liczby parametró modeli\n",
    "3. Jak zwiększanie długości sekwencji wpływa na liczbę parametrów wszystkich modeli? Odpowiedź uzasadnij\n",
    "4. Dlaczego MultiHeadModel ma tyle samo parametrów co AttentionHeadModel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8d7ee-00e2-496a-9f7d-d63e58d9375f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
